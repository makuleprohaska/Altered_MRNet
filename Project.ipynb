{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90776b0f",
   "metadata": {},
   "source": [
    "<h1>Alexnet Majority Vote</h1>\n",
    "We start by recreating the model used in the orgininal MRNet paper to get a baseline for performance\n",
    "This model uses three Alexnet backbones with a dense classification layer trained on axial, coronal and sagittal MRIs respectively and then uses a majority vote system to determine the final output "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b76e02",
   "metadata": {},
   "source": [
    "<h3>Model class</h3>\n",
    "We start by definining the model class "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43cadb6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "from torchvision.models import AlexNet_Weights\n",
    "\n",
    "class MRNet3(nn.Module):\n",
    "    \n",
    "    def __init__(self,use_batchnorm=True):\n",
    "        super().__init__()\n",
    "        self.model1 = models.alexnet(weights=AlexNet_Weights.DEFAULT)\n",
    "        self.model2 = models.alexnet(weights=AlexNet_Weights.DEFAULT)\n",
    "        self.model3 = models.alexnet(weights=AlexNet_Weights.DEFAULT)\n",
    "        self.gap = nn.AdaptiveMaxPool2d(1)\n",
    "        # self.gap = nn.AdaptiveAvgPool2d(1)\n",
    "        self.use_batchnorm = use_batchnorm\n",
    "        n = 0.15\n",
    "        # Dropout for each view's features\n",
    "        self.dropout_view1 = nn.Dropout(p=n)\n",
    "        self.dropout_view2 = nn.Dropout(p=n)\n",
    "        self.dropout_view3 = nn.Dropout(p=n)\n",
    "        \n",
    "        print(f\"Dropout of {n}\")\n",
    "\n",
    "\n",
    "        classifier_layers_axial = [nn.Linear(256, 256)]\n",
    "        if self.use_batchnorm:\n",
    "            classifier_layers_axial.append(nn.BatchNorm1d(256))\n",
    "        self.classifier1_axial = nn.Sequential(*classifier_layers_axial)\n",
    "\n",
    "        classifier_layers_coronal = [nn.Linear(256, 256)]\n",
    "        if self.use_batchnorm:\n",
    "            classifier_layers_coronal.append(nn.BatchNorm1d(256))\n",
    "        self.classifier1_coronal = nn.Sequential(*classifier_layers_coronal)\n",
    "\n",
    "        classifier_layers_sagittal = [nn.Linear(256, 256)]\n",
    "        if self.use_batchnorm:\n",
    "            classifier_layers_sagittal.append(nn.BatchNorm1d(256))\n",
    "        self.classifier1_sagittal = nn.Sequential(*classifier_layers_sagittal)\n",
    "\n",
    "\n",
    "        # Separate classifier2 for each view\n",
    "        self.classifier2_axial = nn.Linear(256, 1)\n",
    "        self.classifier2_coronal = nn.Linear(256, 1)\n",
    "        self.classifier2_sagittal = nn.Linear(256, 1)\n",
    "\n",
    "\n",
    "    #New forward pass to deal with batch normalisation\n",
    "\n",
    "    def forward(self, x): \n",
    "\n",
    "        # Separate by view\n",
    "        axial_views    = [sample[0] for sample in x]\n",
    "        coronal_views  = [sample[1] for sample in x]\n",
    "        sagittal_views = [sample[2] for sample in x]\n",
    "\n",
    "        def process_view(view_list, model, dropout, classifier1, classifier2):\n",
    "            features = []\n",
    "            for view in view_list:\n",
    "                slices, c, h, w = view.size()  # [num_slices, 3, 224, 224]\n",
    "                view = view.view(slices, c, h, w).to(next(model.parameters()).device)\n",
    "                feat = model.features(view)                     # [slices, 256, 6, 6]\n",
    "                feat = self.gap(feat).view(slices, 256)         # [slices, 256]\n",
    "                feat = torch.max(feat, dim=0)[0]                # [256]\n",
    "                feat = dropout(feat)\n",
    "                features.append(feat)\n",
    "            features = torch.stack(features)                    # [batch_size, 256]\n",
    "            features = classifier1(features)                    # [batch_size, 256]\n",
    "            logits = classifier2(features)                      # [batch_size, 1]\n",
    "            return logits\n",
    "\n",
    "        logit_axial    = process_view(axial_views,    self.model1, self.dropout_view1, self.classifier1_axial, self.classifier2_axial)\n",
    "        logit_coronal  = process_view(coronal_views,  self.model2, self.dropout_view2, self.classifier1_coronal, self.classifier2_coronal)\n",
    "        logit_sagittal = process_view(sagittal_views, self.model3, self.dropout_view3, self.classifier1_sagittal, self.classifier2_sagittal)\n",
    "\n",
    "        logits = torch.stack([logit_axial, logit_coronal, logit_sagittal], dim=0)  # [3, batch_size, 1]\n",
    "        probs = torch.sigmoid(logits)                                              # [3, batch_size, 1]\n",
    "        majority_prob = torch.mean(probs, dim=0)                                   # [batch_size, 1]\n",
    "    \n",
    "        return majority_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8283718",
   "metadata": {},
   "source": [
    "<h3>Loader</h3>\n",
    "Then we have the code to correctly load and preprocess the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1fd7b639",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import kornia.augmentation as K\n",
    "import random\n",
    "\n",
    "INPUT_DIM = 224\n",
    "MAX_PIXEL_VAL = 255\n",
    "MEAN = 58.09\n",
    "STDDEV = 49.73\n",
    "\n",
    "class Dataset3(data.Dataset):\n",
    "    def __init__(self, data_dir, file_list, labels_dict, device, train=False, augment=False):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.data_dir_axial = f\"{data_dir}/axial\"\n",
    "        self.data_dir_coronal = f\"{data_dir}/coronal\"\n",
    "        self.data_dir_sagittal = f\"{data_dir}/sagittal\"\n",
    "\n",
    "        self.paths_axial = [os.path.join(self.data_dir_axial, file) for file in file_list]\n",
    "        self.paths_coronal = [os.path.join(self.data_dir_coronal, file) for file in file_list]\n",
    "        self.paths_sagittal = [os.path.join(self.data_dir_sagittal, file) for file in file_list]\n",
    "        \n",
    "        self.paths = [self.paths_axial, self.paths_coronal, self.paths_sagittal]\n",
    "        \n",
    "        self.labels = [labels_dict[file] for file in file_list]\n",
    "\n",
    "        neg_weight = np.mean(self.labels)\n",
    "        self.weights = [neg_weight, 1 - neg_weight]\n",
    "\n",
    "        self.train = train  #this ensures even when augment = True we never perform data augmentation on the validation/test set\n",
    "        self.augment = augment              \n",
    "\n",
    "    def weighted_loss(self, prediction, target, eps: float = 0.0):\n",
    "        # Ensure target is [batch_size, 1]\n",
    "        target = target.view(-1, 1)\n",
    "\n",
    "        # Compute weights for each sample\n",
    "        weights_npy = np.array([self.weights[int(t.item())] for t in target.flatten()])\n",
    "\n",
    "        # Reshape weights to [batch_size, 1] to match prediction and target\n",
    "        weights_tensor = torch.FloatTensor(weights_npy).view(-1, 1).to(target.device)\n",
    "\n",
    "        smoothed = target * (1 - eps) + (1 - target) * eps # new\n",
    "\n",
    "        # 3) compute BCE with logits against the *smoothed* targets\n",
    "        loss = F.binary_cross_entropy_with_logits(prediction, smoothed, weight=weights_tensor) #new\n",
    "        return loss\n",
    "        # # Compute loss with weights reshaped to [batch_size, 1]\n",
    "        # loss = F.binary_cross_entropy_with_logits(prediction, target, weight=weights_tensor)\n",
    "\n",
    "        # return loss\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        vol_list = []\n",
    "        for i in range(3):           \n",
    "            path = self.paths[i][index]\n",
    "            vol = np.load(path).astype(np.int32)\n",
    "            pad = int((vol.shape[2] - INPUT_DIM) / 2)\n",
    "            vol = vol[:, pad:-pad, pad:-pad]\n",
    "            vol = (vol - np.min(vol)) / (np.max(vol) - np.min(vol)) * MAX_PIXEL_VAL\n",
    "            vol = (vol - MEAN) / STDDEV\n",
    "            vol = np.stack((vol,) * 3, axis=1)\n",
    "            vol_tensor = torch.FloatTensor(vol)  # Keep on CPU\n",
    "            vol_list.append(vol_tensor)\n",
    "\n",
    "            # Apply augmentations if train and augment flags are True\n",
    "            if self.train and self.augment:\n",
    "                vol_tensor = self.apply_augmentations(vol_tensor)\n",
    "        \n",
    "            vol_list.append(vol_tensor)\n",
    "\n",
    "        label_tensor = torch.FloatTensor([self.labels[index]])  # Shape: [1]\n",
    "        return vol_list, label_tensor\n",
    "    \n",
    "    def apply_augmentations(self, vol_tensor):\n",
    "        # Apply same augmentations slice-wise\n",
    "        vol_tensor = K.RandomRotation(degrees=25)(vol_tensor)\n",
    "        vol_tensor = K.RandomAffine(degrees=0, translate=(25/224, 25/224))(vol_tensor)\n",
    "        if random.random() > 0.5:\n",
    "            vol_tensor = K.RandomHorizontalFlip(p=1.0)(vol_tensor)\n",
    "        return vol_tensor\n",
    "    \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "def custom_collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Custom collate function to handle variable slice counts.\n",
    "    Returns a list of view tensors and a stacked label tensor.\n",
    "    \"\"\"\n",
    "    vol_lists = [item[0] for item in batch]  # List of [axial, coronal, sagittal] for each sample\n",
    "    labels = torch.stack([item[1] for item in batch], dim=0)  # Stack labels: [batch_size, 1]\n",
    "    return vol_lists, labels\n",
    "\n",
    "def load_data3(device, data_dir, labels_csv, batch_size=1, augment=False):\n",
    "    labels_df = pd.read_csv(labels_csv, header=None, names=['filename', 'label'])\n",
    "    labels_df['filename'] = labels_df['filename'].apply(lambda x: f\"{int(x):04d}.npy\")\n",
    "    \n",
    "    # Filter files that exist in all 3 views\n",
    "    valid_files = []\n",
    "    valid_labels = []\n",
    "    for _, row in labels_df.iterrows():\n",
    "        fname = row['filename']\n",
    "        exists_all_views = all(os.path.exists(os.path.join(data_dir, view, fname)) for view in ['axial', 'coronal', 'sagittal'])\n",
    "        if exists_all_views:\n",
    "            valid_files.append(fname)\n",
    "            valid_labels.append(row['label'])\n",
    "    \n",
    "    labels_dict = dict(zip(valid_files, valid_labels))\n",
    "\n",
    "    # Stratify split\n",
    "    train_files, valid_files = train_test_split(\n",
    "        valid_files,\n",
    "        test_size=0.2,\n",
    "        random_state=42,\n",
    "        stratify=valid_labels\n",
    "    )\n",
    "\n",
    "    train_dataset = Dataset3(data_dir, train_files, labels_dict, device, train=True, augment=augment)\n",
    "    valid_dataset = Dataset3(data_dir, valid_files, labels_dict, device, train=False, augment=False)\n",
    "\n",
    "    train_loader = data.DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=batch_size, \n",
    "        num_workers=0, \n",
    "        shuffle=True, \n",
    "        pin_memory=device.type == 'cuda',\n",
    "        collate_fn=custom_collate_fn\n",
    "    )\n",
    "\n",
    "    valid_loader = data.DataLoader(\n",
    "        valid_dataset, \n",
    "        batch_size=batch_size, \n",
    "        num_workers=0, \n",
    "        shuffle=False, \n",
    "        pin_memory=device.type == 'cuda',\n",
    "        collate_fn=custom_collate_fn\n",
    "    )\n",
    "\n",
    "    print(f\"Training samples: {len(train_dataset)}, Validation samples: {len(valid_dataset)}\")\n",
    "    return train_loader, valid_loader\n",
    "\n",
    "def load_data_test(device, data_dir, labels_csv, batch_size=1, label_smoothing=0):\n",
    "    labels_df = pd.read_csv(labels_csv, header=None, names=['filename', 'label'])\n",
    "    labels_df['filename'] = labels_df['filename'].apply(lambda x: f\"{int(x):04d}.npy\")\n",
    "    labels_dict = dict(zip(labels_df['filename'], labels_df['label']))\n",
    "\n",
    "    test_files = [f for f in os.listdir(f\"{data_dir}/axial\") if f.endswith(\".npy\")]\n",
    "    test_files = [f for f in test_files if f in labels_dict]\n",
    "    test_files.sort()\n",
    "\n",
    "    test_dataset = MRDataset(data_dir, test_files, labels_dict, device, train=False, label_smoothing=label_smoothing, augment=False)\n",
    "\n",
    "    test_loader = data.DataLoader(\n",
    "        test_dataset, \n",
    "        batch_size=batch_size, \n",
    "        num_workers=0, \n",
    "        shuffle=False, \n",
    "        pin_memory=device.type == 'cuda',\n",
    "        collate_fn=collate_fn\n",
    "    )\n",
    "\n",
    "    return test_loader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d868555f",
   "metadata": {},
   "source": [
    "<h3>Training and evaluation functions</h3>\n",
    "Lastly we define the functions to run training and evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "83b06695",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn import metrics\n",
    "from torch.autograd import Variable\n",
    "from tqdm import tqdm\n",
    "\n",
    "def get_device(use_gpu, use_mps):\n",
    "    \n",
    "    if use_gpu and torch.cuda.is_available():\n",
    "        return torch.device(\"cuda\")\n",
    "    \n",
    "    elif use_mps and torch.backends.mps.is_available():\n",
    "        return torch.device(\"mps\")\n",
    "    \n",
    "    else:\n",
    "        return torch.device(\"cpu\")\n",
    "\n",
    "def run_model(model, loader, train=False, optimizer=None, eps: float = 0.0):\n",
    "    \"\"\"\n",
    "    model    : your MRNet3 instance\n",
    "    loader   : DataLoader returning (vol_lists, label)\n",
    "    train    : whether to do optimizer.step()\n",
    "    optimizer: your Adam optimizer (only used if train=True)\n",
    "    eps      : label-smoothing factor (0.0 = no smoothing)\n",
    "    \"\"\"\n",
    "    preds = []\n",
    "    labels = []\n",
    "    total_loss = 0.0\n",
    "    num_batches = 0\n",
    "\n",
    "    if train:\n",
    "        model.train()\n",
    "    else:\n",
    "        model.eval()\n",
    "\n",
    "    device = loader.dataset.device\n",
    "\n",
    "    for vol_lists, label in tqdm(loader, desc=\"Processing batches\", total=len(loader)):\n",
    "        # Move data to device\n",
    "        label = label.to(device)                       # [batch_size,1]\n",
    "        vol_lists = [[view.to(device) for view in views] for views in vol_lists]\n",
    "\n",
    "        # Forward\n",
    "        logits = model(vol_lists)                      # [batch_size,1]\n",
    "        probs  = torch.sigmoid(logits)                 # [batch_size,1]\n",
    "\n",
    "        # Loss\n",
    "        if train:\n",
    "            # use smoothing in training\n",
    "            loss = loader.dataset.weighted_loss(logits, label, eps=eps)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        else:\n",
    "            # no smoothing in val/test\n",
    "            loss = loader.dataset.weighted_loss(logits, label, eps=0.0)\n",
    "\n",
    "        # Accumulate\n",
    "        total_loss += loss.item()\n",
    "        preds.extend(probs.detach().cpu().view(-1).tolist())\n",
    "        labels.extend(label.detach().cpu().view(-1).tolist())\n",
    "        num_batches += 1\n",
    "\n",
    "    avg_loss = total_loss / num_batches\n",
    "    fpr, tpr, _ = metrics.roc_curve(labels, preds)\n",
    "    auc = metrics.auc(fpr, tpr)\n",
    "\n",
    "    return avg_loss, auc, preds, labels\n",
    "\n",
    "def evaluate(split, model_path, use_gpu, mps, data_dir, labels_csv):\n",
    "    device = get_device(use_gpu, mps)\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    \n",
    "    if split == 'test' or split == 'valid':\n",
    "        train_loader, valid_loader = load_data3(device, data_dir, labels_csv)\n",
    "    elif split == 'train':\n",
    "        train_loader = load_data_train(device, data_dir, labels_csv, augment=True)\n",
    "    else:\n",
    "        raise ValueError(\"split must be 'train', 'valid', or 'test'\")\n",
    "    \n",
    "    model = MRNet3()\n",
    "    \n",
    "    state_dict = torch.load(model_path, map_location=device)\n",
    "    model.load_state_dict(state_dict)\n",
    "    model = model.to(device)\n",
    "    \n",
    "    if split == 'train':\n",
    "        loader = train_loader\n",
    "    elif split == 'valid':\n",
    "        loader = valid_loader\n",
    "    elif split == 'test':\n",
    "        loader = test_loader\n",
    "\n",
    "    loss, auc, preds, labels = run_model(model, loader, train=False)\n",
    "    print(f'{split} loss: {loss:.4f}')\n",
    "    print(f'{split} AUC: {auc:.4f}')\n",
    "    return preds, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "47e7abfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from sklearn import metrics\n",
    "\n",
    "def get_device(use_gpu, use_mps):\n",
    "    if use_gpu and torch.cuda.is_available():\n",
    "        return torch.device(\"cuda\")\n",
    "    elif use_mps and torch.backends.mps.is_available():\n",
    "        return torch.device(\"mps\")\n",
    "    else:\n",
    "        return torch.device(\"cpu\")\n",
    "\n",
    "def train3(rundir, epochs, learning_rate, gpu, mps, data_dir, labels_csv, weight_decay, max_patience, batch_size, augment, eps):\n",
    "    device = get_device(gpu, mps)\n",
    "    print(f\"Using device: {device}\")\n",
    "    train_loader, valid_loader = load_data3(device, data_dir, labels_csv, batch_size=batch_size, augment=augment)\n",
    "    \n",
    "    #This now deals with the case that batch size is 1\n",
    "    use_batchnorm = batch_size > 1\n",
    "    model = MRNet3(use_batchnorm=use_batchnorm)\n",
    "    model = model.to(device)\n",
    "\n",
    "    print(f\"Using BatchNorm: {use_batchnorm}\")\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), learning_rate, weight_decay=weight_decay)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=max_patience, factor=.3, threshold=1e-4)\n",
    "\n",
    "    best_val_auc = float('-inf')\n",
    "\n",
    "    start_time = datetime.now()\n",
    "\n",
    "    epsilon = eps\n",
    "    print(f\"Value of eps:{epsilon}\")\n",
    "    for epoch in range(epochs):\n",
    "        change = datetime.now() - start_time\n",
    "        print('starting epoch {}. time passed: {}'.format(epoch+1, str(change)))\n",
    "        \n",
    "        train_loss, train_auc, _, _ = run_model(model, train_loader, train=True, optimizer=optimizer, eps=epsilon)\n",
    "        print(f'train loss: {train_loss:0.4f}')\n",
    "        print(f'train AUC: {train_auc:0.4f}')\n",
    "\n",
    "        val_loss, val_auc, _, _ = run_model(model, valid_loader, eps=0.0)\n",
    "        print(f'valid loss: {val_loss:0.4f}')\n",
    "        print(f'valid AUC: {val_auc:0.4f}')\n",
    "\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        if val_auc > best_val_auc:\n",
    "            best_val_auc = val_auc\n",
    "            file_name = f'val{val_auc:0.4f}_train{train_auc:0.4f}_epoch{epoch+1}'\n",
    "            save_path = Path(rundir) / file_name \n",
    "            torch.save(model.state_dict(), save_path)\n",
    "\n",
    "        # Log metrics to file\n",
    "        with open(os.path.join(rundir, 'metrics.txt'), 'a') as f:\n",
    "            f.write(f\"Epoch {epoch+1}: train_loss={train_loss:.4f}, val_loss={val_loss:.4f}, train_auc={train_auc:.4f}, val_auc={val_auc:.4f}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba0a4b91",
   "metadata": {},
   "source": [
    "<h3>Training</h3>\n",
    "Now we train the model we found the best parameters to be (parameters). As in the original paper, each sample was randomly rotated by an angle between -25 adn 25 degrees, randomly translated by up to 25 pixels and flipped horizontaly with probability 50%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d705d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "Training samples: 904, Validation samples: 226\n",
      "Dropout of 0.15\n",
      "Using BatchNorm: False\n",
      "Value of eps:0.0\n",
      "starting epoch 1. time passed: 0:00:00.000013\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   2%|▏         | 21/904 [00:07<04:54,  3.00it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 45\u001b[39m\n\u001b[32m     42\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(Path(rundir) / \u001b[33m'\u001b[39m\u001b[33margs.json\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mw\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m out:\n\u001b[32m     43\u001b[39m     json.dump(params, out, indent=\u001b[32m4\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m45\u001b[39m     \u001b[43mtrain3\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrundir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     46\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgpu\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels_csv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_patience\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maugment\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 43\u001b[39m, in \u001b[36mtrain3\u001b[39m\u001b[34m(rundir, epochs, learning_rate, gpu, mps, data_dir, labels_csv, weight_decay, max_patience, batch_size, augment, eps)\u001b[39m\n\u001b[32m     40\u001b[39m change = datetime.now() - start_time\n\u001b[32m     41\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mstarting epoch \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m. time passed: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m'\u001b[39m.format(epoch+\u001b[32m1\u001b[39m, \u001b[38;5;28mstr\u001b[39m(change)))\n\u001b[32m---> \u001b[39m\u001b[32m43\u001b[39m train_loss, train_auc, _, _ = \u001b[43mrun_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mepsilon\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     44\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mtrain loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m0.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m)\n\u001b[32m     45\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mtrain AUC: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_auc\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m0.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 62\u001b[39m, in \u001b[36mrun_model\u001b[39m\u001b[34m(model, loader, train, optimizer, eps)\u001b[39m\n\u001b[32m     59\u001b[39m     loss = loader.dataset.weighted_loss(logits, label, eps=\u001b[32m0.0\u001b[39m)\n\u001b[32m     61\u001b[39m \u001b[38;5;66;03m# Accumulate\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m62\u001b[39m total_loss += \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     63\u001b[39m preds.extend(probs.detach().cpu().view(-\u001b[32m1\u001b[39m).tolist())\n\u001b[32m     64\u001b[39m labels.extend(label.detach().cpu().view(-\u001b[32m1\u001b[39m).tolist())\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "rundir =  \"/Users/matteobruno/Desktop/runs\"  #\"directory/to/store/runs\"\n",
    "data_dir = \"/Users/matteobruno/Desktop/MRNet-v1.0/train\" #\"Directory/containing/.npy_files'\"\n",
    "labels_csv =  \"/Users/matteobruno/Desktop/MRNet-v1.0/train/train-acl.csv\" #\"Path/to/labels/CSV/file\"\n",
    "seed = 42\n",
    "gpu = False #If true runs on Nvidia GPU\n",
    "mps = True #If true runs on Apple MPS\n",
    "learning_rate = 1e-05\n",
    "weight_decay = 0.0025\n",
    "epochs = 50\n",
    "max_patience = 5\n",
    "factor = 0.3 \n",
    "batch_size = 1 \n",
    "eps = 0.0 #Label smoothing factor (0.0 = no smoothing)'\n",
    "augment = True  #Apply data augmentation during training\n",
    "\n",
    "\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if gpu and torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "elif mps and torch.backends.mps.is_available():\n",
    "    pass\n",
    "\n",
    "os.makedirs(rundir, exist_ok=True)\n",
    "\n",
    "# Save parameters to args.json\n",
    "params = {\n",
    "    \"rundir\": rundir,\n",
    "    \"data_dir\": data_dir,\n",
    "    \"labels_csv\": labels_csv,\n",
    "    \"seed\": seed,\n",
    "    \"gpu\": gpu,\n",
    "    \"mps\": mps,\n",
    "    \"learning_rate\": learning_rate,\n",
    "    \"weight_decay\": weight_decay,\n",
    "    \"epochs\": epochs,\n",
    "    \"max_patience\": max_patience,\n",
    "    \"batch_size\": batch_size,\n",
    "    \"label_smoothing\": eps,\n",
    "    \"augment\": augment\n",
    "}\n",
    "with open(Path(rundir) / 'args.json', 'w') as out:\n",
    "    json.dump(params, out, indent=4)\n",
    "\n",
    "    train3(rundir, epochs, learning_rate, \n",
    "        gpu, mps, data_dir, labels_csv, weight_decay, max_patience, batch_size, augment, eps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e38975d",
   "metadata": {},
   "source": [
    "<h3>Testing</h3>\n",
    "After training, we test our best model on the test dataset and obtain an AUC of {AUC}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f02bb8ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] --model_path MODEL_PATH --split SPLIT\n",
      "                             --diagnosis DIAGNOSIS --data_dir DATA_DIR\n",
      "                             --labels_csv LABELS_CSV [--gpu] [--mps]\n",
      "ipykernel_launcher.py: error: the following arguments are required: --model_path, --split, --diagnosis, --data_dir, --labels_csv\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[31mSystemExit\u001b[39m\u001b[31m:\u001b[39m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/mrnet/lib/python3.11/site-packages/IPython/core/interactiveshell.py:3557: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "model_path = \"path/to/your/model.pth\"  # Path to the saved model\n",
    "split = \"test\"  # or \"train\", \"valid\"\n",
    "data_dir = \"/Users/matteobruno/Desktop/MRNet-v1.0/test\" #\"Directory/containing/.npy_files'\"\n",
    "labels_csv =  \"/Users/matteobruno/Desktop/MRNet-v1.0/train/train-acl.csv\" #\"Path/to/labels/CSV/file\"\n",
    "gpu = False #If true runs on Nvidia GPU\n",
    "mps = True #If true runs on Apple MPS\n",
    "\n",
    "\n",
    "evaluate(split, model_path, gpu, mps, data_dir, labels_csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea88f9f4",
   "metadata": {},
   "source": [
    "<h1>Resnet</h1>\n",
    "We then decided to try to use a more recent and advanced CNN as a backbone for our model. We opted for Resnet. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "121d9455",
   "metadata": {},
   "source": [
    "<h3>Model class</h3>\n",
    "We start by definining the model class "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "98a14fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "from torchvision.models import ResNet18_Weights\n",
    "\n",
    "class MRNet3(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Initialize ResNet18 backbones (already have BN internally)\n",
    "        self.model1 = models.resnet18(weights=ResNet18_Weights.DEFAULT)\n",
    "        self.model2 = models.resnet18(weights=ResNet18_Weights.DEFAULT)\n",
    "        self.model3 = models.resnet18(weights=ResNet18_Weights.DEFAULT)\n",
    "        \n",
    "        # Remove the original classification layer\n",
    "        self.model1 = nn.Sequential(*list(self.model1.children())[:-1])\n",
    "        self.model2 = nn.Sequential(*list(self.model2.children())[:-1])\n",
    "        self.model3 = nn.Sequential(*list(self.model3.children())[:-1])\n",
    "\n",
    "        self.gap = nn.AdaptiveAvgPool2d(1)  # Global Average Pooling\n",
    "        \n",
    "        # Dropout for each view's features\n",
    "        self.dropout_view1 = nn.Dropout(p=0.7)\n",
    "        self.dropout_view2 = nn.Dropout(p=0.7)\n",
    "        self.dropout_view3 = nn.Dropout(p=0.7)\n",
    "        \n",
    "        # Fully connected layers with batch normalization\n",
    "        self.classifier1 = nn.Linear(512 * 3, 256)  # Concatenated features from 3 views\n",
    "        self.bn1 = nn.BatchNorm1d(256)  # BN after classifier1\n",
    "        self.dropout = nn.Dropout(p=0.4)\n",
    "        self.activation = nn.ReLU()\n",
    "        self.classifier2 = nn.Linear(256, 1)\n",
    "\n",
    "    def forward(self, x, original_slices):\n",
    "        \n",
    "        view_features = []\n",
    "        \n",
    "        for view in range(3):\n",
    "            \n",
    "            x_view = x[view]  # [B, S_max, 3, 224, 224]\n",
    "            B, S_max, _, H, W = x_view.shape\n",
    "            x_view = x_view.view(B * S_max, 3, H, W)\n",
    "            \n",
    "            if view == 0:\n",
    "                features = self.model1(x_view)\n",
    "            elif view == 1:\n",
    "                features = self.model2(x_view)\n",
    "            else:\n",
    "                features = self.model3(x_view)\n",
    "            \n",
    "            features = self.gap(features).view(B, S_max, 512)  # [B, S_max, 512]\n",
    "            s_indices = torch.arange(S_max, device=features.device).unsqueeze(0).expand(B, S_max)\n",
    "            mask = s_indices < original_slices[view].unsqueeze(1)\n",
    "            features = features.masked_fill(~mask.unsqueeze(2), -float('inf'))\n",
    "            max_features = torch.max(features, dim=1)[0]  # [B, 512]\n",
    "            \n",
    "            if view == 0:\n",
    "                max_features = self.dropout_view1(max_features)\n",
    "            elif view == 1:\n",
    "                max_features = self.dropout_view2(max_features)\n",
    "            else:\n",
    "                max_features = self.dropout_view3(max_features)\n",
    "            \n",
    "            view_features.append(max_features)\n",
    "        \n",
    "        # Concatenate features from all views\n",
    "        x_stacked = torch.cat(view_features, dim=1)  # [B, 1536]\n",
    "        \n",
    "        # Fully connected layers with BN\n",
    "        x_stacked = self.classifier1(x_stacked)  # [B, 256]\n",
    "        x_stacked = self.bn1(x_stacked)  # Apply batch normalization\n",
    "        x_stacked = self.dropout(x_stacked)\n",
    "        x_stacked = self.activation(x_stacked)\n",
    "        x_stacked = self.classifier2(x_stacked)  # [B, 1]\n",
    "        \n",
    "        return x_stacked"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd8d752",
   "metadata": {},
   "source": [
    "<h3>Loader</h3>\n",
    "Then we have the code to correctly load and preprocess the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e57690bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "INPUT_DIM = 224\n",
    "MAX_PIXEL_VAL = 1.0  # ResNet expects [0, 1] before channel-wise normalization\n",
    "MEAN = [0.485, 0.456, 0.406]  # ImageNet mean for ResNet (per channel)\n",
    "STDDEV = [0.229, 0.224, 0.225]  # ImageNet std for ResNet (per channel)\n",
    "\n",
    "class MRDataset(data.Dataset):\n",
    "    def __init__(self, data_dir, file_list, labels_dict, device, label_smoothing=0.1):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.data_dir_axial = f\"{data_dir}/axial\"\n",
    "        self.data_dir_coronal = f\"{data_dir}/coronal\"\n",
    "        self.data_dir_sagittal = f\"{data_dir}/sagittal\"\n",
    "\n",
    "        self.paths_axial = [os.path.join(self.data_dir_axial, file) for file in file_list]\n",
    "        self.paths_coronal = [os.path.join(self.data_dir_coronal, file) for file in file_list]\n",
    "        self.paths_sagittal = [os.path.join(self.data_dir_sagittal, file) for file in file_list]\n",
    "        \n",
    "        self.paths = [self.paths_axial, self.paths_coronal, self.paths_sagittal]\n",
    "        \n",
    "        self.labels = [labels_dict[file] for file in file_list]\n",
    "        self.label_smoothing = label_smoothing  # New parameter for label smoothing\n",
    "\n",
    "        neg_weight = np.mean(self.labels)\n",
    "        dtype = np.float32\n",
    "        self.weights = [dtype(neg_weight), dtype(1 - neg_weight)]\n",
    "\n",
    "    def weighted_loss(self, prediction, target, train):\n",
    "        dtype = torch.float32\n",
    "        indices = target.squeeze(1).long()  # Shape: [B]\n",
    "        weights_tensor = torch.tensor(self.weights, device=self.device, dtype=dtype)[indices]  # Shape: [B]\n",
    "        weights_tensor = weights_tensor.unsqueeze(1)  # Shape: [B, 1]\n",
    "\n",
    "        # Apply label smoothing only during training if label_smoothing > 0\n",
    "        if train and self.label_smoothing > 0:\n",
    "            smoothed_target = target * (1 - self.label_smoothing) + (1 - target) * self.label_smoothing\n",
    "        else:\n",
    "            smoothed_target = target\n",
    "\n",
    "        loss = F.binary_cross_entropy_with_logits(prediction, smoothed_target, weight=weights_tensor)\n",
    "        return loss\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        vol_list = []\n",
    "\n",
    "        for i in range(3):           \n",
    "            path = self.paths[i][index]\n",
    "            vol = np.load(path).astype(np.float32) \n",
    "\n",
    "            # Crop to INPUT_DIM x INPUT_DIM (224x224)\n",
    "            pad = int((vol.shape[2] - INPUT_DIM) / 2)\n",
    "            vol = vol[:, pad:-pad, pad:-pad]\n",
    "\n",
    "            # Normalize to [0, 1]\n",
    "            vol = (vol - np.min(vol)) / (np.max(vol) - np.min(vol) + 1e-6)  # [0, 1]\n",
    "\n",
    "            # Stack to 3 channels\n",
    "            vol = np.stack((vol,) * 3, axis=1)  # Shape: (slices, 3, 224, 224)\n",
    "\n",
    "            # Apply ImageNet normalization per channel\n",
    "            vol_tensor = torch.FloatTensor(vol).to(self.device)  # Shape: (slices, 3, 224, 224)\n",
    "            for c in range(3):\n",
    "                vol_tensor[:, c, :, :] = (vol_tensor[:, c, :, :] - MEAN[c]) / STDDEV[c]\n",
    "\n",
    "            vol_list.append(vol_tensor)\n",
    "\n",
    "        label_tensor = torch.FloatTensor([self.labels[index]]).to(self.device)\n",
    "\n",
    "        return vol_list, label_tensor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    device = batch[0][0][0].device\n",
    "    view0_list = [sample[0][0] for sample in batch]  # Axial\n",
    "    view1_list = [sample[0][1] for sample in batch]  # Coronal\n",
    "    view2_list = [sample[0][2] for sample in batch]  # Sagittal\n",
    "    \n",
    "    # Pad slices to the maximum in the batch for each view\n",
    "    padded_view0 = torch.nn.utils.rnn.pad_sequence(view0_list, batch_first=True)\n",
    "    padded_view1 = torch.nn.utils.rnn.pad_sequence(view1_list, batch_first=True)\n",
    "    padded_view2 = torch.nn.utils.rnn.pad_sequence(view2_list, batch_first=True)\n",
    "    \n",
    "    # Store original slice counts for masking in the model\n",
    "    original_slices0 = torch.tensor([v.shape[0] for v in view0_list], device=device)\n",
    "    original_slices1 = torch.tensor([v.shape[0] for v in view1_list], device=device)\n",
    "    original_slices2 = torch.tensor([v.shape[0] for v in view2_list], device=device)\n",
    "    \n",
    "    # Stack labels\n",
    "    labels = torch.stack([sample[1] for sample in batch])\n",
    "    \n",
    "    return [padded_view0, padded_view1, padded_view2], labels, [original_slices0, original_slices1, original_slices2]\n",
    "\n",
    "def load_data3(device, data_dir, labels_csv, batch_size=1, label_smoothing=0.1):\n",
    "    labels_df = pd.read_csv(labels_csv, header=None, names=['filename', 'label'])\n",
    "    labels_df['filename'] = labels_df['filename'].apply(lambda x: f\"{int(x):04d}.npy\")\n",
    "    labels_dict = dict(zip(labels_df['filename'], labels_df['label']))\n",
    "\n",
    "    all_files = [f for f in os.listdir(f\"{data_dir}/axial\") if f.endswith(\".npy\")]\n",
    "    all_files = [f for f in all_files if f in labels_dict]\n",
    "    all_files.sort()\n",
    "\n",
    "    labels = [labels_dict[file] for file in all_files]\n",
    "\n",
    "    train_files, valid_files = train_test_split(\n",
    "        all_files, \n",
    "        test_size=0.2, \n",
    "        random_state=42, \n",
    "        stratify=labels\n",
    "    )\n",
    "\n",
    "    train_dataset = MRDataset(data_dir, train_files, labels_dict, device, label_smoothing=label_smoothing)\n",
    "    valid_dataset = MRDataset(data_dir, valid_files, labels_dict, device, label_smoothing=label_smoothing)\n",
    "\n",
    "    train_loader = data.DataLoader(train_dataset, batch_size=batch_size, num_workers=0, shuffle=True, collate_fn=collate_fn)\n",
    "    valid_loader = data.DataLoader(valid_dataset, batch_size=batch_size, num_workers=0, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "    return train_loader, valid_loader\n",
    "\n",
    "def load_data_test(device, data_dir, labels_csv, batch_size=1, label_smoothing=0):\n",
    "    \n",
    "    labels_df = pd.read_csv(labels_csv, header=None, names=['filename', 'label'])\n",
    "    labels_df['filename'] = labels_df['filename'].apply(lambda x: f\"{int(x):04d}.npy\")\n",
    "    labels_dict = dict(zip(labels_df['filename'], labels_df['label']))\n",
    "\n",
    "    test_files = [f for f in os.listdir(f\"{data_dir}/axial\") if f.endswith(\".npy\")]\n",
    "    test_files = [f for f in test_files if f in labels_dict]\n",
    "    test_files.sort()\n",
    "\n",
    "    test_dataset = MRDataset(data_dir, test_files, labels_dict, device, label_smoothing=label_smoothing)\n",
    "\n",
    "    test_loader = data.DataLoader(test_dataset, batch_size=batch_size, num_workers=0, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "    return test_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "659c12fd",
   "metadata": {},
   "source": [
    "<h3>Training and evaluation functions</h3>\n",
    "Lastly we define the functions to run training and evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ca2ba093",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from sklearn import metrics\n",
    "\n",
    "def get_device(use_gpu, use_mps):\n",
    "    if use_gpu and torch.cuda.is_available():\n",
    "        return torch.device(\"cuda\")\n",
    "    elif use_mps and torch.backends.mps.is_available():\n",
    "        return torch.device(\"mps\")\n",
    "    else:\n",
    "        return torch.device(\"cpu\")\n",
    "\n",
    "def train3(rundir, epochs, learning_rate, use_gpu, use_mps, data_dir, labels_csv, weight_decay, max_patience, batch_size, label_smoothing):\n",
    "    device = get_device(use_gpu, use_mps)\n",
    "    print(f\"Using device: {device}\")\n",
    "    train_loader, valid_loader = load_data3(device, data_dir, labels_csv, batch_size=batch_size, label_smoothing=label_smoothing)\n",
    "    \n",
    "    model = MRNet3()\n",
    "    model = model.to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=max_patience, factor=.3, threshold=1e-4)\n",
    "\n",
    "    best_val_auc = float('-inf')\n",
    "\n",
    "    start_time = datetime.now()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        change = datetime.now() - start_time\n",
    "        print('starting epoch {}. time passed: {}'.format(epoch+1, str(change)))\n",
    "        \n",
    "        train_loss, train_auc, _, _ = run_model(model, train_loader, train=True, optimizer=optimizer)\n",
    "        print(f'train loss: {train_loss:0.4f}')\n",
    "        print(f'train AUC: {train_auc:0.4f}')\n",
    "\n",
    "        val_loss, val_auc, _, _ = run_model(model, valid_loader, train=False)\n",
    "        print(f'valid loss: {val_loss:0.4f}')\n",
    "        print(f'valid AUC: {val_auc:0.4f}')\n",
    "\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        if val_auc > best_val_auc:\n",
    "            best_val_auc = val_auc\n",
    "\n",
    "            file_name = f'val{val_auc:0.4f}_train{train_auc:0.4f}_epoch{epoch+1}'\n",
    "            save_path = Path(rundir) / file_name\n",
    "            \n",
    "            print(f\"Saving model to {save_path}\")\n",
    "            \n",
    "            torch.save(model.state_dict(), save_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "98b6a623",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from sklearn import metrics\n",
    "from torch.autograd import Variable\n",
    "from tqdm import tqdm\n",
    "from torch.cuda.amp import autocast\n",
    "\n",
    "def get_device(use_gpu, use_mps):\n",
    "    if use_gpu and torch.cuda.is_available():\n",
    "        return torch.device(\"cuda\")\n",
    "    elif use_mps and torch.backends.mps.is_available():\n",
    "        return torch.device(\"mps\")\n",
    "    else:\n",
    "        return torch.device(\"cpu\")\n",
    "\n",
    "def run_model(model, loader, train=False, optimizer=None):\n",
    "    preds = []\n",
    "    labels = []\n",
    "\n",
    "    if train:\n",
    "        model.train()\n",
    "    else:\n",
    "        model.eval()\n",
    "\n",
    "    total_loss = 0.\n",
    "    num_batches = 0\n",
    "    print(f\"num_batches: {len(loader)}\")\n",
    "    for batch in tqdm(loader, desc=\"Processing batches\", total=len(loader)):\n",
    "        if train:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        vol, label, original_slices = batch\n",
    "        \n",
    "        vol_device = vol  # List of [B, S_max, 3, 224, 224]\n",
    "        label = label.to(loader.dataset.device)\n",
    "\n",
    "        if str(loader.dataset.device).startswith('cuda'):\n",
    "            with autocast(enabled=True):\n",
    "                logit = model.forward(vol_device, original_slices)\n",
    "                loss = loader.dataset.weighted_loss(logit, label, train)\n",
    "        else:\n",
    "            logit = model.forward(vol_device, original_slices)\n",
    "            loss = loader.dataset.weighted_loss(logit, label, train)\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "\n",
    "        pred = torch.sigmoid(logit)\n",
    "        pred_npy = pred.data.cpu().numpy().flatten()\n",
    "        label_npy = label.data.cpu().numpy().flatten()\n",
    "\n",
    "        preds.extend(pred_npy)\n",
    "        labels.extend(label_npy)\n",
    "\n",
    "        if train:\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        num_batches += 1\n",
    "\n",
    "    avg_loss = total_loss / num_batches\n",
    "\n",
    "    fpr, tpr, threshold = metrics.roc_curve(labels, preds)\n",
    "    auc = metrics.auc(fpr, tpr)\n",
    "\n",
    "    return avg_loss, auc, preds, labels\n",
    "\n",
    "def evaluate(split, model_path, use_gpu, use_mps, data_dir, labels_csv, batch_size, label_smoothing):\n",
    "    device = get_device(use_gpu, use_mps)\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    if split == 'train' or split == 'valid':\n",
    "        train_loader, valid_loader = load_data3(device, data_dir, labels_csv, batch_size=batch_size, label_smoothing=label_smoothing)\n",
    "\n",
    "    elif split == 'test':\n",
    "        test_loader = load_data_test(device, data_dir, labels_csv, batch_size=batch_size, label_smoothing=label_smoothing)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"split must be 'train', 'valid', or 'test'\")\n",
    "    \n",
    "    print(\"Loading model from path:\", model_path)\n",
    "\n",
    "    model = MRNet3()\n",
    "    state_dict = torch.load(model_path, map_location=device)\n",
    "    model.load_state_dict(state_dict)\n",
    "    model = model.to(device)\n",
    "\n",
    "    if split == 'train':\n",
    "        loader = train_loader\n",
    "    elif split == 'valid':\n",
    "        loader = valid_loader\n",
    "    elif split == 'test':\n",
    "        loader = test_loader\n",
    "\n",
    "    loss, auc, preds, labels = run_model(model, loader, train=False)\n",
    "\n",
    "    print(f'{split} loss: {loss:0.4f}')\n",
    "    print(f'{split} AUC: {auc:0.4f}')\n",
    "\n",
    "    return preds, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45417681",
   "metadata": {},
   "source": [
    "<h3>Training</h3>\n",
    "Now we train the model we found the best parameters to be (parameters). Here no data augmentation was used since, when we tried it, it didn't give good results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d6be7f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "starting epoch 1. time passed: 0:00:00.000006\n",
      "num_batches: 452\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   1%|          | 5/452 [00:12<18:31,  2.49s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[34]\u001b[39m\u001b[32m, line 43\u001b[39m\n\u001b[32m     40\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(Path(rundir) / \u001b[33m'\u001b[39m\u001b[33margs.json\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mw\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m out:\n\u001b[32m     41\u001b[39m     json.dump(params, out, indent=\u001b[32m4\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m43\u001b[39m     \u001b[43mtrain3\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrundir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgpu\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels_csv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     44\u001b[39m \u001b[43m           \u001b[49m\u001b[43mmax_patience\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[31]\u001b[39m\u001b[32m, line 38\u001b[39m, in \u001b[36mtrain3\u001b[39m\u001b[34m(rundir, epochs, learning_rate, use_gpu, use_mps, data_dir, labels_csv, weight_decay, max_patience, batch_size, label_smoothing)\u001b[39m\n\u001b[32m     35\u001b[39m change = datetime.now() - start_time\n\u001b[32m     36\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mstarting epoch \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m. time passed: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m'\u001b[39m.format(epoch+\u001b[32m1\u001b[39m, \u001b[38;5;28mstr\u001b[39m(change)))\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m train_loss, train_auc, _, _ = \u001b[43mrun_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     39\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mtrain loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m0.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m)\n\u001b[32m     40\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mtrain AUC: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_auc\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m0.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[29]\u001b[39m\u001b[32m, line 47\u001b[39m, in \u001b[36mrun_model\u001b[39m\u001b[34m(model, loader, train, optimizer)\u001b[39m\n\u001b[32m     45\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     46\u001b[39m     logit = model.forward(vol_device, original_slices)\n\u001b[32m---> \u001b[39m\u001b[32m47\u001b[39m     loss = \u001b[43mloader\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43mweighted_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     49\u001b[39m total_loss += loss.item()\n\u001b[32m     51\u001b[39m pred = torch.sigmoid(logit)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[26]\u001b[39m\u001b[32m, line 38\u001b[39m, in \u001b[36mMRDataset.weighted_loss\u001b[39m\u001b[34m(self, prediction, target, train)\u001b[39m\n\u001b[32m     36\u001b[39m dtype = torch.float32\n\u001b[32m     37\u001b[39m indices = target.squeeze(\u001b[32m1\u001b[39m).long()  \u001b[38;5;66;03m# Shape: [B]\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m weights_tensor = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m[indices]  \u001b[38;5;66;03m# Shape: [B]\u001b[39;00m\n\u001b[32m     39\u001b[39m weights_tensor = weights_tensor.unsqueeze(\u001b[32m1\u001b[39m)  \u001b[38;5;66;03m# Shape: [B, 1]\u001b[39;00m\n\u001b[32m     41\u001b[39m \u001b[38;5;66;03m# Apply label smoothing only during training if label_smoothing > 0\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "\n",
    "rundir =  \"/Users/matteobruno/Desktop/runs\"  #\"directory/to/store/runs\"\n",
    "data_dir = \"/Users/matteobruno/Desktop/MRNet-v1.0/train\" #\"Directory/containing/.npy_files'\"\n",
    "labels_csv =  \"/Users/matteobruno/Desktop/MRNet-v1.0/train/train-acl.csv\" #\"Path/to/labels/CSV/file\"\n",
    "seed = 42\n",
    "gpu = False #If true runs on Nvidia GPU\n",
    "mps = True #If true runs on Apple MPS\n",
    "learning_rate = 1e-04\n",
    "weight_decay = 5e-04\n",
    "epochs = 50\n",
    "max_patience = 5\n",
    "batch_size = 4\n",
    "label_smoothing = 0.0 #Label smoothing factor (0.0 = no smoothing)'\n",
    "\n",
    "\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if gpu and torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "elif mps and torch.backends.mps.is_available():\n",
    "    pass\n",
    "\n",
    "os.makedirs(rundir, exist_ok=True)\n",
    "\n",
    "# Save parameters to args.json\n",
    "params = {\n",
    "    \"rundir\": rundir,\n",
    "    \"data_dir\": data_dir,\n",
    "    \"labels_csv\": labels_csv,\n",
    "    \"seed\": seed,\n",
    "    \"gpu\": gpu,\n",
    "    \"mps\": mps,\n",
    "    \"learning_rate\": learning_rate,\n",
    "    \"weight_decay\": weight_decay,\n",
    "    \"epochs\": epochs,\n",
    "    \"max_patience\": max_patience,\n",
    "    \"batch_size\": batch_size,\n",
    "    \"label_smoothing\": eps,\n",
    "    \"augment\": augment\n",
    "}\n",
    "with open(Path(rundir) / 'args.json', 'w') as out:\n",
    "    json.dump(params, out, indent=4)\n",
    "    \n",
    "    train3(rundir, epochs, learning_rate, gpu, mps, data_dir, labels_csv, weight_decay, \n",
    "           max_patience, batch_size, label_smoothing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f97994b",
   "metadata": {},
   "source": [
    "<h3>Testing</h3>\n",
    "After training, we test our best model on the test dataset and obtain an AUC of {AUC}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "20ccd288",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "Loading model from path: /Users/matteobruno/Desktop/Best_resnet/val0.9547_train0.9974_epoch26\n",
      "num_batches: 120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 100%|██████████| 120/120 [00:32<00:00,  3.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss: 0.1659\n",
      "test AUC: 0.9750\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([np.float32(0.28312284),\n",
       "  np.float32(0.27180526),\n",
       "  np.float32(0.32738855),\n",
       "  np.float32(0.2946371),\n",
       "  np.float32(0.47488645),\n",
       "  np.float32(0.4658895),\n",
       "  np.float32(0.37531993),\n",
       "  np.float32(0.8500066),\n",
       "  np.float32(0.7093115),\n",
       "  np.float32(0.28486842),\n",
       "  np.float32(0.2489606),\n",
       "  np.float32(0.27730957),\n",
       "  np.float32(0.14151964),\n",
       "  np.float32(0.050282598),\n",
       "  np.float32(0.69063985),\n",
       "  np.float32(0.1865429),\n",
       "  np.float32(0.2223076),\n",
       "  np.float32(0.3619938),\n",
       "  np.float32(0.3190657),\n",
       "  np.float32(0.15099639),\n",
       "  np.float32(0.3279567),\n",
       "  np.float32(0.30276978),\n",
       "  np.float32(0.14258036),\n",
       "  np.float32(0.44499302),\n",
       "  np.float32(0.29585764),\n",
       "  np.float32(0.2086031),\n",
       "  np.float32(0.2458953),\n",
       "  np.float32(0.042707503),\n",
       "  np.float32(0.43393013),\n",
       "  np.float32(0.32772425),\n",
       "  np.float32(0.22054808),\n",
       "  np.float32(0.692407),\n",
       "  np.float32(0.49244583),\n",
       "  np.float32(0.5446901),\n",
       "  np.float32(0.13245867),\n",
       "  np.float32(0.13922128),\n",
       "  np.float32(0.33433163),\n",
       "  np.float32(0.23703623),\n",
       "  np.float32(0.57290226),\n",
       "  np.float32(0.4285616),\n",
       "  np.float32(0.15004393),\n",
       "  np.float32(0.49713305),\n",
       "  np.float32(0.88304615),\n",
       "  np.float32(0.5982082),\n",
       "  np.float32(0.83736026),\n",
       "  np.float32(0.89996725),\n",
       "  np.float32(0.88975877),\n",
       "  np.float32(0.74396175),\n",
       "  np.float32(0.83107406),\n",
       "  np.float32(0.875524),\n",
       "  np.float32(0.10670666),\n",
       "  np.float32(0.77635527),\n",
       "  np.float32(0.90238994),\n",
       "  np.float32(0.83148205),\n",
       "  np.float32(0.85323864),\n",
       "  np.float32(0.91045535),\n",
       "  np.float32(0.8168235),\n",
       "  np.float32(0.85536623),\n",
       "  np.float32(0.85678124),\n",
       "  np.float32(0.8750897),\n",
       "  np.float32(0.80802727),\n",
       "  np.float32(0.7441048),\n",
       "  np.float32(0.8658121),\n",
       "  np.float32(0.8802132),\n",
       "  np.float32(0.7858865),\n",
       "  np.float32(0.8436613),\n",
       "  np.float32(0.86744225),\n",
       "  np.float32(0.8178226),\n",
       "  np.float32(0.9072773),\n",
       "  np.float32(0.86635405),\n",
       "  np.float32(0.90278774),\n",
       "  np.float32(0.90442187),\n",
       "  np.float32(0.87845504),\n",
       "  np.float32(0.89684844),\n",
       "  np.float32(0.90741056),\n",
       "  np.float32(0.7430795),\n",
       "  np.float32(0.84170777),\n",
       "  np.float32(0.8934515),\n",
       "  np.float32(0.56621426),\n",
       "  np.float32(0.90075094),\n",
       "  np.float32(0.7882652),\n",
       "  np.float32(0.8202737),\n",
       "  np.float32(0.86918366),\n",
       "  np.float32(0.8430492),\n",
       "  np.float32(0.85422635),\n",
       "  np.float32(0.88888973),\n",
       "  np.float32(0.8875994),\n",
       "  np.float32(0.45195478),\n",
       "  np.float32(0.7738687),\n",
       "  np.float32(0.8703295),\n",
       "  np.float32(0.91628814),\n",
       "  np.float32(0.26184136),\n",
       "  np.float32(0.17333978),\n",
       "  np.float32(0.25137427),\n",
       "  np.float32(0.799738),\n",
       "  np.float32(0.89621603),\n",
       "  np.float32(0.096540414),\n",
       "  np.float32(0.814427),\n",
       "  np.float32(0.882012),\n",
       "  np.float32(0.6893406),\n",
       "  np.float32(0.51588273),\n",
       "  np.float32(0.2808671),\n",
       "  np.float32(0.14914058),\n",
       "  np.float32(0.11201232),\n",
       "  np.float32(0.094921656),\n",
       "  np.float32(0.1827438),\n",
       "  np.float32(0.36247814),\n",
       "  np.float32(0.62052685),\n",
       "  np.float32(0.4496646),\n",
       "  np.float32(0.34742877),\n",
       "  np.float32(0.43562987),\n",
       "  np.float32(0.2693807),\n",
       "  np.float32(0.38523388),\n",
       "  np.float32(0.491215),\n",
       "  np.float32(0.3079988),\n",
       "  np.float32(0.72272086),\n",
       "  np.float32(0.88194126),\n",
       "  np.float32(0.14488247),\n",
       "  np.float32(0.5897152),\n",
       "  np.float32(0.5881555)],\n",
       " [np.float32(0.0),\n",
       "  np.float32(0.0),\n",
       "  np.float32(0.0),\n",
       "  np.float32(0.0),\n",
       "  np.float32(0.0),\n",
       "  np.float32(0.0),\n",
       "  np.float32(0.0),\n",
       "  np.float32(0.0),\n",
       "  np.float32(0.0),\n",
       "  np.float32(0.0),\n",
       "  np.float32(0.0),\n",
       "  np.float32(0.0),\n",
       "  np.float32(0.0),\n",
       "  np.float32(0.0),\n",
       "  np.float32(0.0),\n",
       "  np.float32(0.0),\n",
       "  np.float32(0.0),\n",
       "  np.float32(0.0),\n",
       "  np.float32(0.0),\n",
       "  np.float32(0.0),\n",
       "  np.float32(0.0),\n",
       "  np.float32(0.0),\n",
       "  np.float32(0.0),\n",
       "  np.float32(0.0),\n",
       "  np.float32(0.0),\n",
       "  np.float32(0.0),\n",
       "  np.float32(0.0),\n",
       "  np.float32(0.0),\n",
       "  np.float32(0.0),\n",
       "  np.float32(0.0),\n",
       "  np.float32(0.0),\n",
       "  np.float32(0.0),\n",
       "  np.float32(0.0),\n",
       "  np.float32(0.0),\n",
       "  np.float32(0.0),\n",
       "  np.float32(0.0),\n",
       "  np.float32(0.0),\n",
       "  np.float32(0.0),\n",
       "  np.float32(0.0),\n",
       "  np.float32(0.0),\n",
       "  np.float32(0.0),\n",
       "  np.float32(0.0),\n",
       "  np.float32(1.0),\n",
       "  np.float32(1.0),\n",
       "  np.float32(1.0),\n",
       "  np.float32(1.0),\n",
       "  np.float32(1.0),\n",
       "  np.float32(1.0),\n",
       "  np.float32(1.0),\n",
       "  np.float32(1.0),\n",
       "  np.float32(0.0),\n",
       "  np.float32(1.0),\n",
       "  np.float32(1.0),\n",
       "  np.float32(1.0),\n",
       "  np.float32(1.0),\n",
       "  np.float32(1.0),\n",
       "  np.float32(1.0),\n",
       "  np.float32(1.0),\n",
       "  np.float32(1.0),\n",
       "  np.float32(0.0),\n",
       "  np.float32(1.0),\n",
       "  np.float32(1.0),\n",
       "  np.float32(1.0),\n",
       "  np.float32(1.0),\n",
       "  np.float32(1.0),\n",
       "  np.float32(1.0),\n",
       "  np.float32(1.0),\n",
       "  np.float32(1.0),\n",
       "  np.float32(1.0),\n",
       "  np.float32(1.0),\n",
       "  np.float32(1.0),\n",
       "  np.float32(1.0),\n",
       "  np.float32(1.0),\n",
       "  np.float32(1.0),\n",
       "  np.float32(1.0),\n",
       "  np.float32(1.0),\n",
       "  np.float32(1.0),\n",
       "  np.float32(1.0),\n",
       "  np.float32(1.0),\n",
       "  np.float32(1.0),\n",
       "  np.float32(1.0),\n",
       "  np.float32(1.0),\n",
       "  np.float32(1.0),\n",
       "  np.float32(1.0),\n",
       "  np.float32(1.0),\n",
       "  np.float32(1.0),\n",
       "  np.float32(1.0),\n",
       "  np.float32(0.0),\n",
       "  np.float32(1.0),\n",
       "  np.float32(1.0),\n",
       "  np.float32(1.0),\n",
       "  np.float32(0.0),\n",
       "  np.float32(0.0),\n",
       "  np.float32(0.0),\n",
       "  np.float32(1.0),\n",
       "  np.float32(1.0),\n",
       "  np.float32(0.0),\n",
       "  np.float32(0.0),\n",
       "  np.float32(1.0),\n",
       "  np.float32(1.0),\n",
       "  np.float32(0.0),\n",
       "  np.float32(0.0),\n",
       "  np.float32(0.0),\n",
       "  np.float32(0.0),\n",
       "  np.float32(0.0),\n",
       "  np.float32(0.0),\n",
       "  np.float32(0.0),\n",
       "  np.float32(1.0),\n",
       "  np.float32(0.0),\n",
       "  np.float32(0.0),\n",
       "  np.float32(0.0),\n",
       "  np.float32(0.0),\n",
       "  np.float32(0.0),\n",
       "  np.float32(0.0),\n",
       "  np.float32(0.0),\n",
       "  np.float32(1.0),\n",
       "  np.float32(1.0),\n",
       "  np.float32(0.0),\n",
       "  np.float32(1.0),\n",
       "  np.float32(0.0)])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_path = \"/Users/matteobruno/Desktop/Best_resnet/val0.9547_train0.9974_epoch26\"  # Path to the saved model\n",
    "split = \"test\"  # or \"train\", \"valid\"\n",
    "data_dir = \"/Users/matteobruno/Desktop/MRNet-v1.0/test\" #\"Directory/containing/.npy_files'\"\n",
    "labels_csv =  \"/Users/matteobruno/Desktop/MRNet-v1.0/test/valid-acl.csv\" #\"Path/to/labels/CSV/file\"\n",
    "gpu = False #If true runs on Nvidia GPU\n",
    "mps = True #If true runs on Apple MPS\n",
    "batch_size = 1\n",
    "label_smoothing = 0.0 #Label smoothing factor (0.0 = no smoothing)'\n",
    "\n",
    "evaluate(split, model_path, gpu, mps, data_dir, labels_csv, batch_size, label_smoothing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d4bf91",
   "metadata": {},
   "source": [
    "<h1>Efficientnet</h1>\n",
    "Even after trying a lot of different hyperparamenters (trying different values for learnign rate, weight decay, batch size, dropout and trying to run the model with and without data augmentation) we didn't manage to get a good model.\n",
    "We include it here for completeness "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "638d23cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mrnet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
