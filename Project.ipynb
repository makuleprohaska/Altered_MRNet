{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90776b0f",
   "metadata": {},
   "source": [
    "<h1>Alexnet Majority Vote</h1>\n",
    "We start by recreating the model used in the orgininal MRNet paper to get a baseline for performance\n",
    "This model uses three Alexnet backbones with a dense classification layer trained on axial, coronal and sagittal MRIs respectively and then uses a majority vote system to determine the final output.\n",
    "\n",
    "We found the best parameters to be (parameters). As in the original paper, each sample was randomly rotated by an angle between -25 adn 25 degrees, randomly translated by up to 25 pixels and flipped horizontaly with probability 50%\n",
    "\n",
    "Note for write up: didn't go in depth here since I didn't do it, please add some more details"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b76e02",
   "metadata": {},
   "source": [
    "<h3>Model class</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "43cadb6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "from torchvision.models import AlexNet_Weights\n",
    "\n",
    "class MRNet3(nn.Module):\n",
    "    \n",
    "    def __init__(self,use_batchnorm=False):\n",
    "        super().__init__()\n",
    "        self.model1 = models.alexnet(weights=AlexNet_Weights.DEFAULT)\n",
    "        self.model2 = models.alexnet(weights=AlexNet_Weights.DEFAULT)\n",
    "        self.model3 = models.alexnet(weights=AlexNet_Weights.DEFAULT)\n",
    "        self.gap = nn.AdaptiveMaxPool2d(1)\n",
    "        # self.gap = nn.AdaptiveAvgPool2d(1)\n",
    "        self.use_batchnorm = use_batchnorm\n",
    "        n = 0.15\n",
    "        # Dropout for each view's features\n",
    "        self.dropout_view1 = nn.Dropout(p=n)\n",
    "        self.dropout_view2 = nn.Dropout(p=n)\n",
    "        self.dropout_view3 = nn.Dropout(p=n)\n",
    "        \n",
    "        print(f\"Dropout of {n}\")\n",
    "\n",
    "\n",
    "        classifier_layers_axial = [nn.Linear(256, 256)]\n",
    "        if self.use_batchnorm:\n",
    "            classifier_layers_axial.append(nn.BatchNorm1d(256))\n",
    "        self.classifier1_axial = nn.Sequential(*classifier_layers_axial)\n",
    "\n",
    "        classifier_layers_coronal = [nn.Linear(256, 256)]\n",
    "        if self.use_batchnorm:\n",
    "            classifier_layers_coronal.append(nn.BatchNorm1d(256))\n",
    "        self.classifier1_coronal = nn.Sequential(*classifier_layers_coronal)\n",
    "\n",
    "        classifier_layers_sagittal = [nn.Linear(256, 256)]\n",
    "        if self.use_batchnorm:\n",
    "            classifier_layers_sagittal.append(nn.BatchNorm1d(256))\n",
    "        self.classifier1_sagittal = nn.Sequential(*classifier_layers_sagittal)\n",
    "\n",
    "\n",
    "        # Separate classifier2 for each view\n",
    "        self.classifier2_axial = nn.Linear(256, 1)\n",
    "        self.classifier2_coronal = nn.Linear(256, 1)\n",
    "        self.classifier2_sagittal = nn.Linear(256, 1)\n",
    "\n",
    "\n",
    "    #New forward pass to deal with batch normalisation\n",
    "\n",
    "    def forward(self, x): \n",
    "\n",
    "        # Separate by view\n",
    "        axial_views    = [sample[0] for sample in x]\n",
    "        coronal_views  = [sample[1] for sample in x]\n",
    "        sagittal_views = [sample[2] for sample in x]\n",
    "\n",
    "        def process_view(view_list, model, dropout, classifier1, classifier2):\n",
    "            features = []\n",
    "            for view in view_list:\n",
    "                slices, c, h, w = view.size()  # [num_slices, 3, 224, 224]\n",
    "                view = view.view(slices, c, h, w).to(next(model.parameters()).device)\n",
    "                feat = model.features(view)                     # [slices, 256, 6, 6]\n",
    "                feat = self.gap(feat).view(slices, 256)         # [slices, 256]\n",
    "                feat = torch.max(feat, dim=0)[0]                # [256]\n",
    "                feat = dropout(feat)\n",
    "                features.append(feat)\n",
    "            features = torch.stack(features)                    # [batch_size, 256]\n",
    "            features = classifier1(features)                    # [batch_size, 256]\n",
    "            logits = classifier2(features)                      # [batch_size, 1]\n",
    "            return logits\n",
    "\n",
    "        logit_axial    = process_view(axial_views,    self.model1, self.dropout_view1, self.classifier1_axial, self.classifier2_axial)\n",
    "        logit_coronal  = process_view(coronal_views,  self.model2, self.dropout_view2, self.classifier1_coronal, self.classifier2_coronal)\n",
    "        logit_sagittal = process_view(sagittal_views, self.model3, self.dropout_view3, self.classifier1_sagittal, self.classifier2_sagittal)\n",
    "\n",
    "        logits = torch.stack([logit_axial, logit_coronal, logit_sagittal], dim=0)  # [3, batch_size, 1]\n",
    "        probs = torch.sigmoid(logits)                                              # [3, batch_size, 1]\n",
    "        majority_prob = torch.mean(probs, dim=0)                                   # [batch_size, 1]\n",
    "    \n",
    "        return majority_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8283718",
   "metadata": {},
   "source": [
    "<h3>Loader</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1fd7b639",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import kornia.augmentation as K\n",
    "import random\n",
    "\n",
    "INPUT_DIM = 224\n",
    "MAX_PIXEL_VAL = 255\n",
    "MEAN = 58.09\n",
    "STDDEV = 49.73\n",
    "\n",
    "class Dataset3(data.Dataset):\n",
    "    def __init__(self, data_dir, file_list, labels_dict, device, train=False, augment=False):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.data_dir_axial = f\"{data_dir}/axial\"\n",
    "        self.data_dir_coronal = f\"{data_dir}/coronal\"\n",
    "        self.data_dir_sagittal = f\"{data_dir}/sagittal\"\n",
    "\n",
    "        self.paths_axial = [os.path.join(self.data_dir_axial, file) for file in file_list]\n",
    "        self.paths_coronal = [os.path.join(self.data_dir_coronal, file) for file in file_list]\n",
    "        self.paths_sagittal = [os.path.join(self.data_dir_sagittal, file) for file in file_list]\n",
    "        \n",
    "        self.paths = [self.paths_axial, self.paths_coronal, self.paths_sagittal]\n",
    "        \n",
    "        self.labels = [labels_dict[file] for file in file_list]\n",
    "\n",
    "        neg_weight = np.mean(self.labels)\n",
    "        self.weights = [neg_weight, 1 - neg_weight]\n",
    "\n",
    "        self.train = train  #this ensures even when augment = True we never perform data augmentation on the validation/test set\n",
    "        self.augment = augment              \n",
    "\n",
    "    def weighted_loss(self, prediction, target, eps: float = 0.0):\n",
    "        # Ensure target is [batch_size, 1]\n",
    "        target = target.view(-1, 1)\n",
    "\n",
    "        # Compute weights for each sample\n",
    "        weights_npy = np.array([self.weights[int(t.item())] for t in target.flatten()])\n",
    "\n",
    "        # Reshape weights to [batch_size, 1] to match prediction and target\n",
    "        weights_tensor = torch.FloatTensor(weights_npy).view(-1, 1).to(target.device)\n",
    "\n",
    "        smoothed = target * (1 - eps) + (1 - target) * eps # new\n",
    "\n",
    "        # 3) compute BCE with logits against the *smoothed* targets\n",
    "        loss = F.binary_cross_entropy_with_logits(prediction, smoothed, weight=weights_tensor) #new\n",
    "        return loss\n",
    "        # # Compute loss with weights reshaped to [batch_size, 1]\n",
    "        # loss = F.binary_cross_entropy_with_logits(prediction, target, weight=weights_tensor)\n",
    "\n",
    "        # return loss\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        vol_list = []\n",
    "        for i in range(3):           \n",
    "            path = self.paths[i][index]\n",
    "            vol = np.load(path).astype(np.int32)\n",
    "            pad = int((vol.shape[2] - INPUT_DIM) / 2)\n",
    "            vol = vol[:, pad:-pad, pad:-pad]\n",
    "            vol = (vol - np.min(vol)) / (np.max(vol) - np.min(vol)) * MAX_PIXEL_VAL\n",
    "            vol = (vol - MEAN) / STDDEV\n",
    "            vol = np.stack((vol,) * 3, axis=1)\n",
    "            vol_tensor = torch.FloatTensor(vol)  # Keep on CPU\n",
    "            vol_list.append(vol_tensor)\n",
    "\n",
    "            # Apply augmentations if train and augment flags are True\n",
    "            if self.train and self.augment:\n",
    "                vol_tensor = self.apply_augmentations(vol_tensor)\n",
    "        \n",
    "            vol_list.append(vol_tensor)\n",
    "\n",
    "        label_tensor = torch.FloatTensor([self.labels[index]])  # Shape: [1]\n",
    "        return vol_list, label_tensor\n",
    "    \n",
    "    def apply_augmentations(self, vol_tensor):\n",
    "        # Apply same augmentations slice-wise\n",
    "        vol_tensor = K.RandomRotation(degrees=25)(vol_tensor)\n",
    "        vol_tensor = K.RandomAffine(degrees=0, translate=(25/224, 25/224))(vol_tensor)\n",
    "        if random.random() > 0.5:\n",
    "            vol_tensor = K.RandomHorizontalFlip(p=1.0)(vol_tensor)\n",
    "        return vol_tensor\n",
    "    \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "def custom_collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Custom collate function to handle variable slice counts.\n",
    "    Returns a list of view tensors and a stacked label tensor.\n",
    "    \"\"\"\n",
    "    vol_lists = [item[0] for item in batch]  # List of [axial, coronal, sagittal] for each sample\n",
    "    labels = torch.stack([item[1] for item in batch], dim=0)  # Stack labels: [batch_size, 1]\n",
    "    return vol_lists, labels\n",
    "\n",
    "def load_data3(device, data_dir, labels_csv, batch_size=1, augment=False):\n",
    "    labels_df = pd.read_csv(labels_csv, header=None, names=['filename', 'label'])\n",
    "    labels_df['filename'] = labels_df['filename'].apply(lambda x: f\"{int(x):04d}.npy\")\n",
    "    \n",
    "    # Filter files that exist in all 3 views\n",
    "    valid_files = []\n",
    "    valid_labels = []\n",
    "    for _, row in labels_df.iterrows():\n",
    "        fname = row['filename']\n",
    "        exists_all_views = all(os.path.exists(os.path.join(data_dir, view, fname)) for view in ['axial', 'coronal', 'sagittal'])\n",
    "        if exists_all_views:\n",
    "            valid_files.append(fname)\n",
    "            valid_labels.append(row['label'])\n",
    "    \n",
    "    labels_dict = dict(zip(valid_files, valid_labels))\n",
    "\n",
    "    # Stratify split\n",
    "    train_files, valid_files = train_test_split(\n",
    "        valid_files,\n",
    "        test_size=0.2,\n",
    "        random_state=42,\n",
    "        stratify=valid_labels\n",
    "    )\n",
    "\n",
    "    train_dataset = Dataset3(data_dir, train_files, labels_dict, device, train=True, augment=augment)\n",
    "    valid_dataset = Dataset3(data_dir, valid_files, labels_dict, device, train=False, augment=False)\n",
    "\n",
    "    train_loader = data.DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=batch_size, \n",
    "        num_workers=0, \n",
    "        shuffle=True, \n",
    "        pin_memory=device.type == 'cuda',\n",
    "        collate_fn=custom_collate_fn\n",
    "    )\n",
    "\n",
    "    valid_loader = data.DataLoader(\n",
    "        valid_dataset, \n",
    "        batch_size=batch_size, \n",
    "        num_workers=0, \n",
    "        shuffle=False, \n",
    "        pin_memory=device.type == 'cuda',\n",
    "        collate_fn=custom_collate_fn\n",
    "    )\n",
    "\n",
    "    print(f\"Training samples: {len(train_dataset)}, Validation samples: {len(valid_dataset)}\")\n",
    "    return train_loader, valid_loader\n",
    "\n",
    "def load_data_test(device, data_dir, labels_csv, batch_size=1):\n",
    "    \n",
    "    labels_df = pd.read_csv(labels_csv, header=None, names=['filename', 'label'])\n",
    "    labels_df['filename'] = labels_df['filename'].apply(lambda x: f\"{int(x):04d}.npy\")\n",
    "    \n",
    "    # Filter files that exist in all 3 views\n",
    "    test_files = []\n",
    "    test_labels = []\n",
    "    for _, row in labels_df.iterrows():\n",
    "        fname = row['filename']\n",
    "        exists_all_views = all(os.path.exists(os.path.join(data_dir, view, fname)) for view in ['axial', 'coronal', 'sagittal'])\n",
    "        if exists_all_views:\n",
    "            test_files.append(fname)\n",
    "            test_labels.append(row['label'])\n",
    "    \n",
    "    labels_dict = dict(zip(test_files, test_labels))\n",
    "\n",
    "    test_dataset = Dataset3(data_dir, test_files, labels_dict, device, train=False, augment=False)\n",
    "\n",
    "    test_loader = data.DataLoader(\n",
    "        test_dataset, \n",
    "        batch_size=batch_size, \n",
    "        num_workers=0, \n",
    "        shuffle=False, \n",
    "        pin_memory=device.type == 'cuda',\n",
    "        collate_fn=custom_collate_fn\n",
    "    )\n",
    "    return test_loader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d868555f",
   "metadata": {},
   "source": [
    "<h3>Training and evaluation functions</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "83b06695",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn import metrics\n",
    "from torch.autograd import Variable\n",
    "from tqdm import tqdm\n",
    "\n",
    "def get_device(use_gpu, use_mps):\n",
    "    \n",
    "    if use_gpu and torch.cuda.is_available():\n",
    "        return torch.device(\"cuda\")\n",
    "    \n",
    "    elif use_mps and torch.backends.mps.is_available():\n",
    "        return torch.device(\"mps\")\n",
    "    \n",
    "    else:\n",
    "        return torch.device(\"cpu\")\n",
    "\n",
    "def run_model(model, loader, train=False, optimizer=None, eps: float = 0.0):\n",
    "    \"\"\"\n",
    "    model    : your MRNet3 instance\n",
    "    loader   : DataLoader returning (vol_lists, label)\n",
    "    train    : whether to do optimizer.step()\n",
    "    optimizer: your Adam optimizer (only used if train=True)\n",
    "    eps      : label-smoothing factor (0.0 = no smoothing)\n",
    "    \"\"\"\n",
    "    preds = []\n",
    "    labels = []\n",
    "    total_loss = 0.0\n",
    "    num_batches = 0\n",
    "\n",
    "    if train:\n",
    "        model.train()\n",
    "    else:\n",
    "        model.eval()\n",
    "\n",
    "    device = loader.dataset.device\n",
    "\n",
    "    for vol_lists, label in tqdm(loader, desc=\"Processing batches\", total=len(loader)):\n",
    "        # Move data to device\n",
    "        label = label.to(device)                       # [batch_size,1]\n",
    "        vol_lists = [[view.to(device) for view in views] for views in vol_lists]\n",
    "\n",
    "        # Forward\n",
    "        logits = model(vol_lists)                      # [batch_size,1]\n",
    "        probs  = torch.sigmoid(logits)                 # [batch_size,1]\n",
    "\n",
    "        # Loss\n",
    "        if train:\n",
    "            # use smoothing in training\n",
    "            loss = loader.dataset.weighted_loss(logits, label, eps=eps)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        else:\n",
    "            # no smoothing in val/test\n",
    "            loss = loader.dataset.weighted_loss(logits, label, eps=0.0)\n",
    "\n",
    "        # Accumulate\n",
    "        total_loss += loss.item()\n",
    "        preds.extend(probs.detach().cpu().view(-1).tolist())\n",
    "        labels.extend(label.detach().cpu().view(-1).tolist())\n",
    "        num_batches += 1\n",
    "\n",
    "    avg_loss = total_loss / num_batches\n",
    "    fpr, tpr, _ = metrics.roc_curve(labels, preds)\n",
    "    auc = metrics.auc(fpr, tpr)\n",
    "\n",
    "    return avg_loss, auc, preds, labels\n",
    "\n",
    "def evaluate(split, model_path, use_gpu, mps, data_dir, labels_csv):\n",
    "    device = get_device(use_gpu, mps)\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    \n",
    "    if split == 'train' or split == 'valid':\n",
    "        train_loader, valid_loader = load_data3(device, data_dir, labels_csv)\n",
    "    elif split == 'test':\n",
    "        test_loader = load_data_test(device, data_dir, labels_csv)\n",
    "    else:\n",
    "        raise ValueError(\"split must be 'train', 'valid', or 'test'\")\n",
    "    \n",
    "    model = MRNet3()\n",
    "    \n",
    "    state_dict = torch.load(model_path, map_location=device)\n",
    "    model.load_state_dict(state_dict)\n",
    "    model = model.to(device)\n",
    "    \n",
    "    if split == 'train':\n",
    "        loader = train_loader\n",
    "    elif split == 'valid':\n",
    "        loader = valid_loader\n",
    "    elif split == 'test':\n",
    "        loader = test_loader\n",
    "\n",
    "    loss, auc, preds, labels = run_model(model, loader, train=False)\n",
    "    print(f'{split} loss: {loss:.4f}')\n",
    "    print(f'{split} AUC: {auc:.4f}')\n",
    "    return preds, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "47e7abfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from sklearn import metrics\n",
    "\n",
    "def get_device(use_gpu, use_mps):\n",
    "    if use_gpu and torch.cuda.is_available():\n",
    "        return torch.device(\"cuda\")\n",
    "    elif use_mps and torch.backends.mps.is_available():\n",
    "        return torch.device(\"mps\")\n",
    "    else:\n",
    "        return torch.device(\"cpu\")\n",
    "\n",
    "def train3(rundir, epochs, learning_rate, gpu, mps, data_dir, labels_csv, weight_decay, max_patience, batch_size, augment, eps):\n",
    "    device = get_device(gpu, mps)\n",
    "    print(f\"Using device: {device}\")\n",
    "    train_loader, valid_loader = load_data3(device, data_dir, labels_csv, batch_size=batch_size, augment=augment)\n",
    "    \n",
    "    #This now deals with the case that batch size is 1\n",
    "    use_batchnorm = batch_size > 1\n",
    "    model = MRNet3(use_batchnorm=use_batchnorm)\n",
    "    model = model.to(device)\n",
    "\n",
    "    print(f\"Using BatchNorm: {use_batchnorm}\")\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), learning_rate, weight_decay=weight_decay)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=max_patience, factor=.3, threshold=1e-4)\n",
    "\n",
    "    best_val_auc = float('-inf')\n",
    "\n",
    "    start_time = datetime.now()\n",
    "\n",
    "    epsilon = eps\n",
    "    print(f\"Value of eps:{epsilon}\")\n",
    "    for epoch in range(epochs):\n",
    "        change = datetime.now() - start_time\n",
    "        print('starting epoch {}. time passed: {}'.format(epoch+1, str(change)))\n",
    "        \n",
    "        train_loss, train_auc, _, _ = run_model(model, train_loader, train=True, optimizer=optimizer, eps=epsilon)\n",
    "        print(f'train loss: {train_loss:0.4f}')\n",
    "        print(f'train AUC: {train_auc:0.4f}')\n",
    "\n",
    "        val_loss, val_auc, _, _ = run_model(model, valid_loader, eps=0.0)\n",
    "        print(f'valid loss: {val_loss:0.4f}')\n",
    "        print(f'valid AUC: {val_auc:0.4f}')\n",
    "\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        if val_auc > best_val_auc:\n",
    "            best_val_auc = val_auc\n",
    "            file_name = f'val{val_auc:0.4f}_train{train_auc:0.4f}_epoch{epoch+1}'\n",
    "            save_path = Path(rundir) / file_name \n",
    "            torch.save(model.state_dict(), save_path)\n",
    "\n",
    "        # Log metrics to file\n",
    "        with open(os.path.join(rundir, 'metrics.txt'), 'a') as f:\n",
    "            f.write(f\"Epoch {epoch+1}: train_loss={train_loss:.4f}, val_loss={val_loss:.4f}, train_auc={train_auc:.4f}, val_auc={val_auc:.4f}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba0a4b91",
   "metadata": {},
   "source": [
    "<h3>Training</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "41d705d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "Training samples: 904, Validation samples: 226\n",
      "Dropout of 0.15\n",
      "Using BatchNorm: False\n",
      "Value of eps:0.0\n",
      "starting epoch 1. time passed: 0:00:00.000017\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   2%|▏         | 17/904 [00:04<03:52,  3.82it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 45\u001b[39m\n\u001b[32m     42\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(Path(rundir) / \u001b[33m'\u001b[39m\u001b[33margs.json\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mw\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m out:\n\u001b[32m     43\u001b[39m     json.dump(params, out, indent=\u001b[32m4\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m45\u001b[39m     \u001b[43mtrain3\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrundir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     46\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgpu\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels_csv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_patience\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maugment\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[26]\u001b[39m\u001b[32m, line 43\u001b[39m, in \u001b[36mtrain3\u001b[39m\u001b[34m(rundir, epochs, learning_rate, gpu, mps, data_dir, labels_csv, weight_decay, max_patience, batch_size, augment, eps)\u001b[39m\n\u001b[32m     40\u001b[39m change = datetime.now() - start_time\n\u001b[32m     41\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mstarting epoch \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m. time passed: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m'\u001b[39m.format(epoch+\u001b[32m1\u001b[39m, \u001b[38;5;28mstr\u001b[39m(change)))\n\u001b[32m---> \u001b[39m\u001b[32m43\u001b[39m train_loss, train_auc, _, _ = \u001b[43mrun_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mepsilon\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     44\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mtrain loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m0.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m)\n\u001b[32m     45\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mtrain AUC: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_auc\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m0.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 41\u001b[39m, in \u001b[36mrun_model\u001b[39m\u001b[34m(model, loader, train, optimizer, eps)\u001b[39m\n\u001b[32m     37\u001b[39m     model.eval()\n\u001b[32m     39\u001b[39m device = loader.dataset.device\n\u001b[32m---> \u001b[39m\u001b[32m41\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mvol_lists\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdesc\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mProcessing batches\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mloader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     42\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Move data to device\u001b[39;49;00m\n\u001b[32m     43\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m                       \u001b[49m\u001b[38;5;66;43;03m# [batch_size,1]\u001b[39;49;00m\n\u001b[32m     44\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvol_lists\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[43mview\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mview\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mviews\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mviews\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mvol_lists\u001b[49m\u001b[43m]\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/mrnet/lib/python3.11/site-packages/tqdm/std.py:1181\u001b[39m, in \u001b[36mtqdm.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1178\u001b[39m time = \u001b[38;5;28mself\u001b[39m._time\n\u001b[32m   1180\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1181\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1182\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[32m   1183\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[32m   1184\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/mrnet/lib/python3.11/site-packages/torch/utils/data/dataloader.py:733\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    730\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    731\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    732\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m733\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    734\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    735\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    736\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    737\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    738\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    739\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/mrnet/lib/python3.11/site-packages/torch/utils/data/dataloader.py:789\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    787\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    788\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m789\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    790\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    791\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/mrnet/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     50\u001b[39m         data = \u001b[38;5;28mself\u001b[39m.dataset.__getitems__(possibly_batched_index)\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m         data = \u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/mrnet/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[39m, in \u001b[36m<listcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m     50\u001b[39m         data = \u001b[38;5;28mself\u001b[39m.dataset.__getitems__(possibly_batched_index)\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m         data = [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 67\u001b[39m, in \u001b[36mDataset3.__getitem__\u001b[39m\u001b[34m(self, index)\u001b[39m\n\u001b[32m     65\u001b[39m vol = (vol - np.min(vol)) / (np.max(vol) - np.min(vol)) * MAX_PIXEL_VAL\n\u001b[32m     66\u001b[39m vol = (vol - MEAN) / STDDEV\n\u001b[32m---> \u001b[39m\u001b[32m67\u001b[39m vol = \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     68\u001b[39m vol_tensor = torch.FloatTensor(vol)  \u001b[38;5;66;03m# Keep on CPU\u001b[39;00m\n\u001b[32m     69\u001b[39m vol_list.append(vol_tensor)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/mrnet/lib/python3.11/site-packages/numpy/_core/shape_base.py:467\u001b[39m, in \u001b[36mstack\u001b[39m\u001b[34m(arrays, axis, out, dtype, casting)\u001b[39m\n\u001b[32m    465\u001b[39m sl = (\u001b[38;5;28mslice\u001b[39m(\u001b[38;5;28;01mNone\u001b[39;00m),) * axis + (_nx.newaxis,)\n\u001b[32m    466\u001b[39m expanded_arrays = [arr[sl] \u001b[38;5;28;01mfor\u001b[39;00m arr \u001b[38;5;129;01min\u001b[39;00m arrays]\n\u001b[32m--> \u001b[39m\u001b[32m467\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_nx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexpanded_arrays\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    468\u001b[39m \u001b[43m                       \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcasting\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcasting\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "rundir =  \"/Users/matteobruno/Desktop/runs\"  #\"directory/to/store/runs\"\n",
    "data_dir = \"/Users/matteobruno/Desktop/models_and_data/MRNet-v1.0/train\" #\"Directory/containing/.npy_files'\"\n",
    "labels_csv =  \"/Users/matteobruno/Desktop/models_and_data/MRNet-v1.0/train/train-acl.csv\" #\"Path/to/labels/CSV/file\"\n",
    "seed = 42\n",
    "gpu = False #If true runs on Nvidia GPU\n",
    "mps = True #If true runs on Apple MPS\n",
    "learning_rate = 1e-05\n",
    "weight_decay = 0.0025\n",
    "epochs = 50\n",
    "max_patience = 5\n",
    "factor = 0.3 \n",
    "batch_size = 1 \n",
    "eps = 0.0 #Label smoothing factor (0.0 = no smoothing)'\n",
    "augment = True  #Apply data augmentation during training\n",
    "\n",
    "\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if gpu and torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "elif mps and torch.backends.mps.is_available():\n",
    "    pass\n",
    "\n",
    "os.makedirs(rundir, exist_ok=True)\n",
    "\n",
    "# Save parameters to args.json\n",
    "params = {\n",
    "    \"rundir\": rundir,\n",
    "    \"data_dir\": data_dir,\n",
    "    \"labels_csv\": labels_csv,\n",
    "    \"seed\": seed,\n",
    "    \"gpu\": gpu,\n",
    "    \"mps\": mps,\n",
    "    \"learning_rate\": learning_rate,\n",
    "    \"weight_decay\": weight_decay,\n",
    "    \"epochs\": epochs,\n",
    "    \"max_patience\": max_patience,\n",
    "    \"batch_size\": batch_size,\n",
    "    \"label_smoothing\": eps,\n",
    "    \"augment\": augment\n",
    "}\n",
    "with open(Path(rundir) / 'args.json', 'w') as out:\n",
    "    json.dump(params, out, indent=4)\n",
    "\n",
    "    train3(rundir, epochs, learning_rate, \n",
    "        gpu, mps, data_dir, labels_csv, weight_decay, max_patience, batch_size, augment, eps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e38975d",
   "metadata": {},
   "source": [
    "<h3>Testing</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f02bb8ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "Dropout of 0.15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 100%|██████████| 120/120 [00:18<00:00,  6.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss: 0.3050\n",
      "test AUC: 0.9495\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([0.5079914331436157,\n",
       "  0.501662015914917,\n",
       "  0.534428596496582,\n",
       "  0.5015807151794434,\n",
       "  0.534299910068512,\n",
       "  0.512273907661438,\n",
       "  0.5107388496398926,\n",
       "  0.6554552316665649,\n",
       "  0.5007240176200867,\n",
       "  0.5014246702194214,\n",
       "  0.5126606822013855,\n",
       "  0.5208551287651062,\n",
       "  0.5003081560134888,\n",
       "  0.5004255175590515,\n",
       "  0.5149188041687012,\n",
       "  0.5026782751083374,\n",
       "  0.5003659725189209,\n",
       "  0.5553420782089233,\n",
       "  0.5008665323257446,\n",
       "  0.5003727674484253,\n",
       "  0.5336424112319946,\n",
       "  0.5008522868156433,\n",
       "  0.5007810592651367,\n",
       "  0.5008218884468079,\n",
       "  0.5069884657859802,\n",
       "  0.5008457899093628,\n",
       "  0.5152267217636108,\n",
       "  0.5016493201255798,\n",
       "  0.5043213963508606,\n",
       "  0.50148606300354,\n",
       "  0.5004546046257019,\n",
       "  0.5328283309936523,\n",
       "  0.5932879447937012,\n",
       "  0.5320087671279907,\n",
       "  0.5007878541946411,\n",
       "  0.502911388874054,\n",
       "  0.5011802315711975,\n",
       "  0.5090853571891785,\n",
       "  0.5007158517837524,\n",
       "  0.5008416771888733,\n",
       "  0.5638201236724854,\n",
       "  0.5247063636779785,\n",
       "  0.5924491286277771,\n",
       "  0.5798209309577942,\n",
       "  0.6506409049034119,\n",
       "  0.583593487739563,\n",
       "  0.6603497862815857,\n",
       "  0.5850343108177185,\n",
       "  0.5584318041801453,\n",
       "  0.6595838069915771,\n",
       "  0.5102148056030273,\n",
       "  0.6185443997383118,\n",
       "  0.6530240178108215,\n",
       "  0.5196832418441772,\n",
       "  0.5791395306587219,\n",
       "  0.6537885665893555,\n",
       "  0.6560285091400146,\n",
       "  0.5729033350944519,\n",
       "  0.6336579918861389,\n",
       "  0.6279531121253967,\n",
       "  0.6010310649871826,\n",
       "  0.5805447101593018,\n",
       "  0.6416690945625305,\n",
       "  0.6065699458122253,\n",
       "  0.6203176975250244,\n",
       "  0.6516116857528687,\n",
       "  0.5340754389762878,\n",
       "  0.6598685383796692,\n",
       "  0.6609655022621155,\n",
       "  0.5817552208900452,\n",
       "  0.6199255585670471,\n",
       "  0.6533766388893127,\n",
       "  0.6602701544761658,\n",
       "  0.6505038142204285,\n",
       "  0.6599313020706177,\n",
       "  0.6291429400444031,\n",
       "  0.6041094064712524,\n",
       "  0.6487771272659302,\n",
       "  0.5794546008110046,\n",
       "  0.6373138427734375,\n",
       "  0.5851190090179443,\n",
       "  0.6281832456588745,\n",
       "  0.5852605104446411,\n",
       "  0.5725818276405334,\n",
       "  0.6431915163993835,\n",
       "  0.6526690721511841,\n",
       "  0.5725325345993042,\n",
       "  0.5003888010978699,\n",
       "  0.5407251119613647,\n",
       "  0.5647315382957458,\n",
       "  0.6572538018226624,\n",
       "  0.5023411512374878,\n",
       "  0.5540214776992798,\n",
       "  0.574152946472168,\n",
       "  0.599683403968811,\n",
       "  0.6514015197753906,\n",
       "  0.5004395246505737,\n",
       "  0.5969188213348389,\n",
       "  0.6169008612632751,\n",
       "  0.5294679999351501,\n",
       "  0.58014976978302,\n",
       "  0.5172387361526489,\n",
       "  0.507004976272583,\n",
       "  0.5223641991615295,\n",
       "  0.5030563473701477,\n",
       "  0.5459871888160706,\n",
       "  0.5012333989143372,\n",
       "  0.5961197018623352,\n",
       "  0.5009141564369202,\n",
       "  0.5015284419059753,\n",
       "  0.5622199773788452,\n",
       "  0.529041588306427,\n",
       "  0.5361645221710205,\n",
       "  0.523953378200531,\n",
       "  0.5011957287788391,\n",
       "  0.600724995136261,\n",
       "  0.613917887210846,\n",
       "  0.5004410743713379,\n",
       "  0.5946277976036072,\n",
       "  0.5039824843406677],\n",
       " [0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  0.0])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_path = \"/Users/matteobruno/Desktop/models_and_data/Best_alexnet_majority/best_model.pth\"  # Path to the saved model\n",
    "split = \"test\"  # or \"train\", \"valid\"\n",
    "data_dir = \"/Users/matteobruno/Desktop/models_and_data/MRNet-v1.0/test\" #\"Directory/containing/.npy_files'\"\n",
    "labels_csv =  \"/Users/matteobruno/Desktop/models_and_data/MRNet-v1.0/test/valid-acl.csv\" #\"Path/to/labels/CSV/file\"\n",
    "gpu = False #If true runs on Nvidia GPU\n",
    "mps = True #If true runs on Apple MPS\n",
    "\n",
    "\n",
    "evaluate(split, model_path, gpu, mps, data_dir, labels_csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea88f9f4",
   "metadata": {},
   "source": [
    "<h1>Resnet</h1>\n",
    "We then decided to try to use a more recent and advanced CNN as a backbone for our model. We opted for Resnet.\n",
    "Other than switching from three Alexnet backbones to three Resnet backbones, we also made some other changes to improve AUC and training speed:\n",
    "\n",
    "- The forward pass process all slices at once instead of using a for loop. Since views may have different number of slices, they are padded to the max number of slices in the sample and then a mask is used to consider only the origina slices. This allows for significantly faster trainign \n",
    "\n",
    "- Instead of using a majority vote sistem, the classification layer is a two layers deep MLP\n",
    "\n",
    "The final version of the model has been trained with the following setup:\n",
    "\n",
    "- No data augmentation, since it is computationally expensive and didn't seem to provide any significant benefit\n",
    "\n",
    "- Strong dropout of 0.7 after feature extraction to prevent the significant overfetting that we were initially experiencing \n",
    "\n",
    "- Lable smoothing with a factor of 0.1 to further regularize the model \n",
    "\n",
    "- Batch size of 4 since it was the largest that could fit in memory\n",
    "\n",
    "The other parameters can be seen in the training cell.\n",
    "\n",
    "We obtained a validation AUC of 95.47% (train AUC of 99.74%) and a testing AUC of 97.5%, likely due to some luck and ot the small size of the sample test "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "121d9455",
   "metadata": {},
   "source": [
    "<h3>Model class</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a14fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "from torchvision.models import ResNet18_Weights\n",
    "\n",
    "class MRNet3(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Initialize ResNet18 backbones (already have BN internally)\n",
    "        self.model1 = models.resnet18(weights=ResNet18_Weights.DEFAULT)\n",
    "        self.model2 = models.resnet18(weights=ResNet18_Weights.DEFAULT)\n",
    "        self.model3 = models.resnet18(weights=ResNet18_Weights.DEFAULT)\n",
    "        \n",
    "        # Remove the original classification layer\n",
    "        self.model1 = nn.Sequential(*list(self.model1.children())[:-1])\n",
    "        self.model2 = nn.Sequential(*list(self.model2.children())[:-1])\n",
    "        self.model3 = nn.Sequential(*list(self.model3.children())[:-1])\n",
    "\n",
    "        self.gap = nn.AdaptiveAvgPool2d(1)  # Global Average Pooling\n",
    "        \n",
    "        # Dropout for each view's features\n",
    "        self.dropout_view1 = nn.Dropout(p=0.7)\n",
    "        self.dropout_view2 = nn.Dropout(p=0.7)\n",
    "        self.dropout_view3 = nn.Dropout(p=0.7)\n",
    "        \n",
    "        # Fully connected layers with batch normalization\n",
    "        self.classifier1 = nn.Linear(512 * 3, 256)  # Concatenated features from 3 views\n",
    "        self.bn1 = nn.BatchNorm1d(256)  # BN after classifier1\n",
    "        self.dropout = nn.Dropout(p=0.4)\n",
    "        self.activation = nn.ReLU()\n",
    "        self.classifier2 = nn.Linear(256, 1)\n",
    "\n",
    "    def forward(self, x, original_slices):\n",
    "        \n",
    "        view_features = []\n",
    "        \n",
    "        for view in range(3):\n",
    "            \n",
    "            x_view = x[view]  # [B, S_max, 3, 224, 224]\n",
    "            B, S_max, _, H, W = x_view.shape\n",
    "            x_view = x_view.view(B * S_max, 3, H, W)\n",
    "            \n",
    "            if view == 0:\n",
    "                features = self.model1(x_view)\n",
    "            elif view == 1:\n",
    "                features = self.model2(x_view)\n",
    "            else:\n",
    "                features = self.model3(x_view)\n",
    "            \n",
    "            features = self.gap(features).view(B, S_max, 512)  # [B, S_max, 512]\n",
    "            s_indices = torch.arange(S_max, device=features.device).unsqueeze(0).expand(B, S_max)\n",
    "            mask = s_indices < original_slices[view].unsqueeze(1)\n",
    "            features = features.masked_fill(~mask.unsqueeze(2), -float('inf'))\n",
    "            max_features = torch.max(features, dim=1)[0]  # [B, 512]\n",
    "            \n",
    "            if view == 0:\n",
    "                max_features = self.dropout_view1(max_features)\n",
    "            elif view == 1:\n",
    "                max_features = self.dropout_view2(max_features)\n",
    "            else:\n",
    "                max_features = self.dropout_view3(max_features)\n",
    "            \n",
    "            view_features.append(max_features)\n",
    "        \n",
    "        # Concatenate features from all views\n",
    "        x_stacked = torch.cat(view_features, dim=1)  # [B, 1536]\n",
    "        \n",
    "        # Fully connected layers with BN\n",
    "        x_stacked = self.classifier1(x_stacked)  # [B, 256]\n",
    "        x_stacked = self.bn1(x_stacked)  # Apply batch normalization\n",
    "        x_stacked = self.dropout(x_stacked)\n",
    "        x_stacked = self.activation(x_stacked)\n",
    "        x_stacked = self.classifier2(x_stacked)  # [B, 1]\n",
    "        \n",
    "        return x_stacked"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd8d752",
   "metadata": {},
   "source": [
    "<h3>Loader</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e57690bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "INPUT_DIM = 224\n",
    "MAX_PIXEL_VAL = 1.0  # ResNet expects [0, 1] before channel-wise normalization\n",
    "MEAN = [0.485, 0.456, 0.406]  # ImageNet mean for ResNet (per channel)\n",
    "STDDEV = [0.229, 0.224, 0.225]  # ImageNet std for ResNet (per channel)\n",
    "\n",
    "class MRDataset(data.Dataset):\n",
    "    def __init__(self, data_dir, file_list, labels_dict, device, label_smoothing=0.1):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.data_dir_axial = f\"{data_dir}/axial\"\n",
    "        self.data_dir_coronal = f\"{data_dir}/coronal\"\n",
    "        self.data_dir_sagittal = f\"{data_dir}/sagittal\"\n",
    "\n",
    "        self.paths_axial = [os.path.join(self.data_dir_axial, file) for file in file_list]\n",
    "        self.paths_coronal = [os.path.join(self.data_dir_coronal, file) for file in file_list]\n",
    "        self.paths_sagittal = [os.path.join(self.data_dir_sagittal, file) for file in file_list]\n",
    "        \n",
    "        self.paths = [self.paths_axial, self.paths_coronal, self.paths_sagittal]\n",
    "        \n",
    "        self.labels = [labels_dict[file] for file in file_list]\n",
    "        self.label_smoothing = label_smoothing  # New parameter for label smoothing\n",
    "\n",
    "        neg_weight = np.mean(self.labels)\n",
    "        dtype = np.float32\n",
    "        self.weights = [dtype(neg_weight), dtype(1 - neg_weight)]\n",
    "\n",
    "    def weighted_loss(self, prediction, target, train):\n",
    "        dtype = torch.float32\n",
    "        indices = target.squeeze(1).long()  # Shape: [B]\n",
    "        weights_tensor = torch.tensor(self.weights, device=self.device, dtype=dtype)[indices]  # Shape: [B]\n",
    "        weights_tensor = weights_tensor.unsqueeze(1)  # Shape: [B, 1]\n",
    "\n",
    "        # Apply label smoothing only during training if label_smoothing > 0\n",
    "        if train and self.label_smoothing > 0:\n",
    "            smoothed_target = target * (1 - self.label_smoothing) + (1 - target) * self.label_smoothing\n",
    "        else:\n",
    "            smoothed_target = target\n",
    "\n",
    "        loss = F.binary_cross_entropy_with_logits(prediction, smoothed_target, weight=weights_tensor)\n",
    "        return loss\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        vol_list = []\n",
    "\n",
    "        for i in range(3):           \n",
    "            path = self.paths[i][index]\n",
    "            vol = np.load(path).astype(np.float32) \n",
    "\n",
    "            # Crop to INPUT_DIM x INPUT_DIM (224x224)\n",
    "            pad = int((vol.shape[2] - INPUT_DIM) / 2)\n",
    "            vol = vol[:, pad:-pad, pad:-pad]\n",
    "\n",
    "            # Normalize to [0, 1]\n",
    "            vol = (vol - np.min(vol)) / (np.max(vol) - np.min(vol) + 1e-6)  # [0, 1]\n",
    "\n",
    "            # Stack to 3 channels\n",
    "            vol = np.stack((vol,) * 3, axis=1)  # Shape: (slices, 3, 224, 224)\n",
    "\n",
    "            # Apply ImageNet normalization per channel\n",
    "            vol_tensor = torch.FloatTensor(vol).to(self.device)  # Shape: (slices, 3, 224, 224)\n",
    "            for c in range(3):\n",
    "                vol_tensor[:, c, :, :] = (vol_tensor[:, c, :, :] - MEAN[c]) / STDDEV[c]\n",
    "\n",
    "            vol_list.append(vol_tensor)\n",
    "\n",
    "        label_tensor = torch.FloatTensor([self.labels[index]]).to(self.device)\n",
    "\n",
    "        return vol_list, label_tensor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    device = batch[0][0][0].device\n",
    "    view0_list = [sample[0][0] for sample in batch]  # Axial\n",
    "    view1_list = [sample[0][1] for sample in batch]  # Coronal\n",
    "    view2_list = [sample[0][2] for sample in batch]  # Sagittal\n",
    "    \n",
    "    # Pad slices to the maximum in the batch for each view\n",
    "    padded_view0 = torch.nn.utils.rnn.pad_sequence(view0_list, batch_first=True)\n",
    "    padded_view1 = torch.nn.utils.rnn.pad_sequence(view1_list, batch_first=True)\n",
    "    padded_view2 = torch.nn.utils.rnn.pad_sequence(view2_list, batch_first=True)\n",
    "    \n",
    "    # Store original slice counts for masking in the model\n",
    "    original_slices0 = torch.tensor([v.shape[0] for v in view0_list], device=device)\n",
    "    original_slices1 = torch.tensor([v.shape[0] for v in view1_list], device=device)\n",
    "    original_slices2 = torch.tensor([v.shape[0] for v in view2_list], device=device)\n",
    "    \n",
    "    # Stack labels\n",
    "    labels = torch.stack([sample[1] for sample in batch])\n",
    "    \n",
    "    return [padded_view0, padded_view1, padded_view2], labels, [original_slices0, original_slices1, original_slices2]\n",
    "\n",
    "def load_data3(device, data_dir, labels_csv, batch_size=1, label_smoothing=0.1):\n",
    "    labels_df = pd.read_csv(labels_csv, header=None, names=['filename', 'label'])\n",
    "    labels_df['filename'] = labels_df['filename'].apply(lambda x: f\"{int(x):04d}.npy\")\n",
    "    labels_dict = dict(zip(labels_df['filename'], labels_df['label']))\n",
    "\n",
    "    all_files = [f for f in os.listdir(f\"{data_dir}/axial\") if f.endswith(\".npy\")]\n",
    "    all_files = [f for f in all_files if f in labels_dict]\n",
    "    all_files.sort()\n",
    "\n",
    "    labels = [labels_dict[file] for file in all_files]\n",
    "\n",
    "    train_files, valid_files = train_test_split(\n",
    "        all_files, \n",
    "        test_size=0.2, \n",
    "        random_state=42, \n",
    "        stratify=labels\n",
    "    )\n",
    "\n",
    "    train_dataset = MRDataset(data_dir, train_files, labels_dict, device, label_smoothing=label_smoothing)\n",
    "    valid_dataset = MRDataset(data_dir, valid_files, labels_dict, device, label_smoothing=label_smoothing)\n",
    "\n",
    "    train_loader = data.DataLoader(train_dataset, batch_size=batch_size, num_workers=0, shuffle=True, collate_fn=collate_fn)\n",
    "    valid_loader = data.DataLoader(valid_dataset, batch_size=batch_size, num_workers=0, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "    return train_loader, valid_loader\n",
    "\n",
    "def load_data_test(device, data_dir, labels_csv, batch_size=1, label_smoothing=0):\n",
    "    \n",
    "    labels_df = pd.read_csv(labels_csv, header=None, names=['filename', 'label'])\n",
    "    labels_df['filename'] = labels_df['filename'].apply(lambda x: f\"{int(x):04d}.npy\")\n",
    "    labels_dict = dict(zip(labels_df['filename'], labels_df['label']))\n",
    "\n",
    "    test_files = [f for f in os.listdir(f\"{data_dir}/axial\") if f.endswith(\".npy\")]\n",
    "    test_files = [f for f in test_files if f in labels_dict]\n",
    "    test_files.sort()\n",
    "\n",
    "    test_dataset = MRDataset(data_dir, test_files, labels_dict, device, label_smoothing=label_smoothing)\n",
    "\n",
    "    test_loader = data.DataLoader(test_dataset, batch_size=batch_size, num_workers=0, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "    return test_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "659c12fd",
   "metadata": {},
   "source": [
    "<h3>Training and evaluation functions</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca2ba093",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from sklearn import metrics\n",
    "\n",
    "def get_device(use_gpu, use_mps):\n",
    "    if use_gpu and torch.cuda.is_available():\n",
    "        return torch.device(\"cuda\")\n",
    "    elif use_mps and torch.backends.mps.is_available():\n",
    "        return torch.device(\"mps\")\n",
    "    else:\n",
    "        return torch.device(\"cpu\")\n",
    "\n",
    "def train3(rundir, epochs, learning_rate, use_gpu, use_mps, data_dir, labels_csv, weight_decay, max_patience, batch_size, label_smoothing):\n",
    "    device = get_device(use_gpu, use_mps)\n",
    "    print(f\"Using device: {device}\")\n",
    "    train_loader, valid_loader = load_data3(device, data_dir, labels_csv, batch_size=batch_size, label_smoothing=label_smoothing)\n",
    "    \n",
    "    model = MRNet3()\n",
    "    model = model.to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=max_patience, factor=.3, threshold=1e-4)\n",
    "\n",
    "    best_val_auc = float('-inf')\n",
    "\n",
    "    start_time = datetime.now()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        change = datetime.now() - start_time\n",
    "        print('starting epoch {}. time passed: {}'.format(epoch+1, str(change)))\n",
    "        \n",
    "        train_loss, train_auc, _, _ = run_model(model, train_loader, train=True, optimizer=optimizer)\n",
    "        print(f'train loss: {train_loss:0.4f}')\n",
    "        print(f'train AUC: {train_auc:0.4f}')\n",
    "\n",
    "        val_loss, val_auc, _, _ = run_model(model, valid_loader, train=False)\n",
    "        print(f'valid loss: {val_loss:0.4f}')\n",
    "        print(f'valid AUC: {val_auc:0.4f}')\n",
    "\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        if val_auc > best_val_auc:\n",
    "            best_val_auc = val_auc\n",
    "\n",
    "            file_name = f'val{val_auc:0.4f}_train{train_auc:0.4f}_epoch{epoch+1}'\n",
    "            save_path = Path(rundir) / file_name\n",
    "            \n",
    "            print(f\"Saving model to {save_path}\")\n",
    "            \n",
    "            torch.save(model.state_dict(), save_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b6a623",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from sklearn import metrics\n",
    "from torch.autograd import Variable\n",
    "from tqdm import tqdm\n",
    "from torch.cuda.amp import autocast\n",
    "\n",
    "def get_device(use_gpu, use_mps):\n",
    "    if use_gpu and torch.cuda.is_available():\n",
    "        return torch.device(\"cuda\")\n",
    "    elif use_mps and torch.backends.mps.is_available():\n",
    "        return torch.device(\"mps\")\n",
    "    else:\n",
    "        return torch.device(\"cpu\")\n",
    "\n",
    "def run_model(model, loader, train=False, optimizer=None):\n",
    "    preds = []\n",
    "    labels = []\n",
    "\n",
    "    if train:\n",
    "        model.train()\n",
    "    else:\n",
    "        model.eval()\n",
    "\n",
    "    total_loss = 0.\n",
    "    num_batches = 0\n",
    "    print(f\"num_batches: {len(loader)}\")\n",
    "    for batch in tqdm(loader, desc=\"Processing batches\", total=len(loader)):\n",
    "        if train:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        vol, label, original_slices = batch\n",
    "        \n",
    "        vol_device = vol  # List of [B, S_max, 3, 224, 224]\n",
    "        label = label.to(loader.dataset.device)\n",
    "\n",
    "        if str(loader.dataset.device).startswith('cuda'):\n",
    "            with autocast(enabled=True):\n",
    "                logit = model.forward(vol_device, original_slices)\n",
    "                loss = loader.dataset.weighted_loss(logit, label, train)\n",
    "        else:\n",
    "            logit = model.forward(vol_device, original_slices)\n",
    "            loss = loader.dataset.weighted_loss(logit, label, train)\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "\n",
    "        pred = torch.sigmoid(logit)\n",
    "        pred_npy = pred.data.cpu().numpy().flatten()\n",
    "        label_npy = label.data.cpu().numpy().flatten()\n",
    "\n",
    "        preds.extend(pred_npy)\n",
    "        labels.extend(label_npy)\n",
    "\n",
    "        if train:\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        num_batches += 1\n",
    "\n",
    "    avg_loss = total_loss / num_batches\n",
    "\n",
    "    fpr, tpr, threshold = metrics.roc_curve(labels, preds)\n",
    "    auc = metrics.auc(fpr, tpr)\n",
    "\n",
    "    return avg_loss, auc, preds, labels\n",
    "\n",
    "def evaluate(split, model_path, use_gpu, use_mps, data_dir, labels_csv, batch_size, label_smoothing):\n",
    "    device = get_device(use_gpu, use_mps)\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    if split == 'train' or split == 'valid':\n",
    "        train_loader, valid_loader = load_data3(device, data_dir, labels_csv, batch_size=batch_size, label_smoothing=label_smoothing)\n",
    "\n",
    "    elif split == 'test':\n",
    "        test_loader = load_data_test(device, data_dir, labels_csv, batch_size=batch_size, label_smoothing=label_smoothing)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"split must be 'train', 'valid', or 'test'\")\n",
    "    \n",
    "    print(\"Loading model from path:\", model_path)\n",
    "\n",
    "    model = MRNet3()\n",
    "    state_dict = torch.load(model_path, map_location=device)\n",
    "    model.load_state_dict(state_dict)\n",
    "    model = model.to(device)\n",
    "\n",
    "    if split == 'train':\n",
    "        loader = train_loader\n",
    "    elif split == 'valid':\n",
    "        loader = valid_loader\n",
    "    elif split == 'test':\n",
    "        loader = test_loader\n",
    "\n",
    "    loss, auc, preds, labels = run_model(model, loader, train=False)\n",
    "\n",
    "    print(f'{split} loss: {loss:0.4f}')\n",
    "    print(f'{split} AUC: {auc:0.4f}')\n",
    "\n",
    "    return preds, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45417681",
   "metadata": {},
   "source": [
    "<h3>Training</h3>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d6be7f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rundir =  \"/Users/matteobruno/Desktop/runs\"  #\"directory/to/store/runs\"\n",
    "data_dir = \"/Users/matteobruno/Desktop/MRNet-v1.0/train\" #\"Directory/containing/.npy_files'\"\n",
    "labels_csv =  \"/Users/matteobruno/Desktop/MRNet-v1.0/train/train-acl.csv\" #\"Path/to/labels/CSV/file\"\n",
    "seed = 42\n",
    "gpu = False #If true runs on Nvidia GPU\n",
    "mps = True #If true runs on Apple MPS\n",
    "learning_rate = 1e-04\n",
    "weight_decay = 5e-04\n",
    "epochs = 50\n",
    "max_patience = 5\n",
    "batch_size = 4\n",
    "label_smoothing = 0.1 #Label smoothing factor (0.0 = no smoothing)'\n",
    "\n",
    "\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if gpu and torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "elif mps and torch.backends.mps.is_available():\n",
    "    pass\n",
    "\n",
    "os.makedirs(rundir, exist_ok=True)\n",
    "\n",
    "# Save parameters to args.json\n",
    "params = {\n",
    "    \"rundir\": rundir,\n",
    "    \"data_dir\": data_dir,\n",
    "    \"labels_csv\": labels_csv,\n",
    "    \"seed\": seed,\n",
    "    \"gpu\": gpu,\n",
    "    \"mps\": mps,\n",
    "    \"learning_rate\": learning_rate,\n",
    "    \"weight_decay\": weight_decay,\n",
    "    \"epochs\": epochs,\n",
    "    \"max_patience\": max_patience,\n",
    "    \"batch_size\": batch_size,\n",
    "    \"label_smoothing\": label_smoothing,\n",
    "}\n",
    "with open(Path(rundir) / 'args.json', 'w') as out:\n",
    "    json.dump(params, out, indent=4)\n",
    "    \n",
    "    train3(rundir, epochs, learning_rate, gpu, mps, data_dir, labels_csv, weight_decay, \n",
    "           max_patience, batch_size, label_smoothing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f97994b",
   "metadata": {},
   "source": [
    "<h3>Testing</h3>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ccd288",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"/Users/matteobruno/Desktop/Best_resnet/val0.9547_train0.9974_epoch26\"  # Path to the saved model\n",
    "split = \"test\"  # or \"train\", \"valid\"\n",
    "data_dir = \"/Users/matteobruno/Desktop/MRNet-v1.0/test\" #\"Directory/containing/.npy_files'\"\n",
    "labels_csv =  \"/Users/matteobruno/Desktop/MRNet-v1.0/test/valid-acl.csv\" #\"Path/to/labels/CSV/file\"\n",
    "gpu = False #If true runs on Nvidia GPU\n",
    "mps = True #If true runs on Apple MPS\n",
    "batch_size = 1\n",
    "label_smoothing = 0.0 #Label smoothing factor (0.0 = no smoothing)'\n",
    "\n",
    "evaluate(split, model_path, gpu, mps, data_dir, labels_csv, batch_size, label_smoothing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d4bf91",
   "metadata": {},
   "source": [
    "<h1>Efficientnet</h1>\n",
    "Even after trying a lot of different hyperparamenters (trying different values for learnign rate, weight decay, batch size, dropout and trying to run the model with and without data augmentation) we didn't manage to get a good model.\n",
    "We include it here for completeness "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb06788",
   "metadata": {},
   "source": [
    "<h3>Model class</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "638d23cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "from torchvision.models import EfficientNet_B0_Weights\n",
    "\n",
    "class MRNet3(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "            \n",
    "        # Load pretrained EfficientNet-B0 from torchvision\n",
    "        self.model1 = models.efficientnet_b0(weights=EfficientNet_B0_Weights.DEFAULT)\n",
    "        self.model2 = models.efficientnet_b0(weights=EfficientNet_B0_Weights.DEFAULT)\n",
    "        self.model3 = models.efficientnet_b0(weights=EfficientNet_B0_Weights.DEFAULT)\n",
    "        \n",
    "        # Remove the classifier head to get feature extractor\n",
    "        self.model1.classifier = nn.Identity()  # EfficientNet-B0 outputs 1280 features\n",
    "        self.model2.classifier = nn.Identity()\n",
    "        self.model3.classifier = nn.Identity()\n",
    "        \n",
    "        # Enhanced fully connected classifier\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(1280 * 3, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 1)\n",
    "            )\n",
    "\n",
    "    def forward(self, x, original_slices):\n",
    "        view_features = []\n",
    "        \n",
    "        for view in range(3):\n",
    "            x_view = x[view]  # [B, S_max, 3, 224, 224]\n",
    "            B, S_max, _, H, W = x_view.shape\n",
    "            x_view = x_view.view(B * S_max, 3, H, W)\n",
    "            \n",
    "            if view == 0:\n",
    "                features = self.model1(x_view)  # [B * S_max, 1280]\n",
    "            elif view == 1:\n",
    "                features = self.model2(x_view)\n",
    "            else:\n",
    "                features = self.model3(x_view)\n",
    "            \n",
    "            features = features.view(B, S_max, 1280)  # [B, S_max, 1280]\n",
    "            s_indices = torch.arange(S_max, device=features.device).unsqueeze(0).expand(B, S_max)\n",
    "            mask = s_indices < original_slices[view].unsqueeze(1)\n",
    "            features = features.masked_fill(~mask.unsqueeze(2), -float('inf'))\n",
    "            max_features = torch.max(features, dim=1)[0]  # [B, 1280]\n",
    "            \n",
    "            view_features.append(max_features)\n",
    "        \n",
    "        # Concatenate features from all views\n",
    "        x_stacked = torch.cat(view_features, dim=1)  # [B, 1280 * 3 = 3840]\n",
    "        \n",
    "        # Pass through the enhanced classifier\n",
    "        output = self.classifier(x_stacked)  # [B, 1]\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1b70d46",
   "metadata": {},
   "source": [
    "<h3>Loader</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "555d7baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "INPUT_DIM = 224\n",
    "MAX_PIXEL_VAL = 1.0  \n",
    "MEAN = [0.485, 0.456, 0.406] \n",
    "STDDEV = [0.229, 0.224, 0.225]  \n",
    "\n",
    "class MRDataset(data.Dataset):\n",
    "    def __init__(self, data_dir, file_list, labels_dict, device, label_smoothing=0.1):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.data_dir_axial = f\"{data_dir}/axial\"\n",
    "        self.data_dir_coronal = f\"{data_dir}/coronal\"\n",
    "        self.data_dir_sagittal = f\"{data_dir}/sagittal\"\n",
    "\n",
    "        self.paths_axial = [os.path.join(self.data_dir_axial, file) for file in file_list]\n",
    "        self.paths_coronal = [os.path.join(self.data_dir_coronal, file) for file in file_list]\n",
    "        self.paths_sagittal = [os.path.join(self.data_dir_sagittal, file) for file in file_list]\n",
    "        \n",
    "        self.paths = [self.paths_axial, self.paths_coronal, self.paths_sagittal]\n",
    "        \n",
    "        self.labels = [labels_dict[file] for file in file_list]\n",
    "        self.label_smoothing = label_smoothing  # New parameter for label smoothing\n",
    "\n",
    "        neg_weight = np.mean(self.labels)\n",
    "        dtype = np.float32\n",
    "        self.weights = [dtype(neg_weight), dtype(1 - neg_weight)]\n",
    "\n",
    "    def weighted_loss(self, prediction, target, train):\n",
    "        dtype = torch.float32\n",
    "        indices = target.squeeze(1).long()  # Shape: [B]\n",
    "        weights_tensor = torch.tensor(self.weights, device=self.device, dtype=dtype)[indices]  # Shape: [B]\n",
    "        weights_tensor = weights_tensor.unsqueeze(1)  # Shape: [B, 1]\n",
    "\n",
    "        # Apply label smoothing only during training if label_smoothing > 0\n",
    "        if train and self.label_smoothing > 0:\n",
    "            smoothed_target = target * (1 - self.label_smoothing) + (1 - target) * self.label_smoothing\n",
    "        else:\n",
    "            smoothed_target = target\n",
    "\n",
    "        loss = F.binary_cross_entropy_with_logits(prediction, smoothed_target, weight=weights_tensor)\n",
    "        return loss\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        vol_list = []\n",
    "\n",
    "        for i in range(3):           \n",
    "            path = self.paths[i][index]\n",
    "            vol = np.load(path).astype(np.float32) \n",
    "\n",
    "            # Crop to INPUT_DIM x INPUT_DIM (224x224)\n",
    "            pad = int((vol.shape[2] - INPUT_DIM) / 2)\n",
    "            vol = vol[:, pad:-pad, pad:-pad]\n",
    "\n",
    "            # Normalize to [0, 1]\n",
    "            vol = (vol - np.min(vol)) / (np.max(vol) - np.min(vol) + 1e-6)  # [0, 1]\n",
    "\n",
    "            # Stack to 3 channels\n",
    "            vol = np.stack((vol,) * 3, axis=1)  # Shape: (slices, 3, 224, 224)\n",
    "\n",
    "            # Apply ImageNet normalization per channel\n",
    "            vol_tensor = torch.FloatTensor(vol).to(self.device)  # Shape: (slices, 3, 224, 224)\n",
    "            for c in range(3):\n",
    "                vol_tensor[:, c, :, :] = (vol_tensor[:, c, :, :] - MEAN[c]) / STDDEV[c]\n",
    "\n",
    "            vol_list.append(vol_tensor)\n",
    "\n",
    "        label_tensor = torch.FloatTensor([self.labels[index]]).to(self.device)\n",
    "\n",
    "        return vol_list, label_tensor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    device = batch[0][0][0].device\n",
    "    view0_list = [sample[0][0] for sample in batch]  # Axial\n",
    "    view1_list = [sample[0][1] for sample in batch]  # Coronal\n",
    "    view2_list = [sample[0][2] for sample in batch]  # Sagittal\n",
    "    \n",
    "    # Pad slices to the maximum in the batch for each view\n",
    "    padded_view0 = torch.nn.utils.rnn.pad_sequence(view0_list, batch_first=True)\n",
    "    padded_view1 = torch.nn.utils.rnn.pad_sequence(view1_list, batch_first=True)\n",
    "    padded_view2 = torch.nn.utils.rnn.pad_sequence(view2_list, batch_first=True)\n",
    "    \n",
    "    # Store original slice counts for masking in the model\n",
    "    original_slices0 = torch.tensor([v.shape[0] for v in view0_list], device=device)\n",
    "    original_slices1 = torch.tensor([v.shape[0] for v in view1_list], device=device)\n",
    "    original_slices2 = torch.tensor([v.shape[0] for v in view2_list], device=device)\n",
    "    \n",
    "    # Stack labels\n",
    "    labels = torch.stack([sample[1] for sample in batch])\n",
    "    \n",
    "    return [padded_view0, padded_view1, padded_view2], labels, [original_slices0, original_slices1, original_slices2]\n",
    "\n",
    "def load_data3(device, data_dir, labels_csv, batch_size=1, label_smoothing=0.1):\n",
    "    labels_df = pd.read_csv(labels_csv, header=None, names=['filename', 'label'])\n",
    "    labels_df['filename'] = labels_df['filename'].apply(lambda x: f\"{int(x):04d}.npy\")\n",
    "    labels_dict = dict(zip(labels_df['filename'], labels_df['label']))\n",
    "\n",
    "    all_files = [f for f in os.listdir(f\"{data_dir}/axial\") if f.endswith(\".npy\")]\n",
    "    all_files = [f for f in all_files if f in labels_dict]\n",
    "    all_files.sort()\n",
    "\n",
    "    labels = [labels_dict[file] for file in all_files]\n",
    "\n",
    "    train_files, valid_files = train_test_split(\n",
    "        all_files, \n",
    "        test_size=0.2, \n",
    "        random_state=42, \n",
    "        stratify=labels\n",
    "    )\n",
    "\n",
    "    train_dataset = MRDataset(data_dir, train_files, labels_dict, device, label_smoothing=label_smoothing)\n",
    "    valid_dataset = MRDataset(data_dir, valid_files, labels_dict, device, label_smoothing=label_smoothing)\n",
    "\n",
    "    train_loader = data.DataLoader(train_dataset, batch_size=batch_size, num_workers=0, shuffle=True, collate_fn=collate_fn)\n",
    "    valid_loader = data.DataLoader(valid_dataset, batch_size=batch_size, num_workers=0, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "    return train_loader, valid_loader\n",
    "\n",
    "def load_data_test(device, data_dir, labels_csv, batch_size=1, label_smoothing=0):\n",
    "    \n",
    "    labels_df = pd.read_csv(labels_csv, header=None, names=['filename', 'label'])\n",
    "    labels_df['filename'] = labels_df['filename'].apply(lambda x: f\"{int(x):04d}.npy\")\n",
    "    labels_dict = dict(zip(labels_df['filename'], labels_df['label']))\n",
    "\n",
    "    test_files = [f for f in os.listdir(f\"{data_dir}/axial\") if f.endswith(\".npy\")]\n",
    "    test_files = [f for f in test_files if f in labels_dict]\n",
    "    test_files.sort()\n",
    "\n",
    "    test_dataset = MRDataset(data_dir, test_files, labels_dict, device, label_smoothing=label_smoothing)\n",
    "\n",
    "    test_loader = data.DataLoader(test_dataset, batch_size=batch_size, num_workers=0, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "    return test_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "517e5574",
   "metadata": {},
   "source": [
    "<h3>Training and evaluation functions</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f6b8f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from sklearn import metrics\n",
    "from torch.autograd import Variable\n",
    "from tqdm import tqdm\n",
    "from torch.cuda.amp import autocast\n",
    "\n",
    "def get_device(use_gpu, use_mps):\n",
    "    if use_gpu and torch.cuda.is_available():\n",
    "        return torch.device(\"cuda\")\n",
    "    elif use_mps and torch.backends.mps.is_available():\n",
    "        return torch.device(\"mps\")\n",
    "    else:\n",
    "        return torch.device(\"cpu\")\n",
    "\n",
    "def run_model(model, loader, train=False, optimizer=None, accumulation_steps=4):\n",
    "    \"\"\"\n",
    "    Run the model on the given data loader with gradient accumulation for training.\n",
    "    \n",
    "    Args:\n",
    "        model: The neural network model.\n",
    "        loader: DataLoader providing the batches.\n",
    "        train: Boolean indicating training or evaluation mode.\n",
    "        optimizer: Optimizer used for weight updates (required if train=True).\n",
    "        accumulation_steps: Number of batches to accumulate gradients over (default: 4).\n",
    "    \n",
    "    Returns:\n",
    "        avg_loss: Average loss over the dataset.\n",
    "        auc: Area under the ROC curve.\n",
    "        preds: List of predictions.\n",
    "        labels: List of true labels.\n",
    "    \"\"\"\n",
    "    preds = []\n",
    "    labels = []\n",
    "    total_loss = 0.\n",
    "    num_batches = 0\n",
    "\n",
    "    # Set model mode\n",
    "    if train:\n",
    "        model.train()\n",
    "    else:\n",
    "        model.eval()\n",
    "\n",
    "    # Process batches\n",
    "    for i, batch in enumerate(tqdm(loader, desc=\"Processing batches\", total=len(loader))):\n",
    "        vol, label, original_slices = batch\n",
    "        vol_device = vol  # Assuming vol is already on the correct device\n",
    "        label = label.to(loader.dataset.device)\n",
    "\n",
    "        # Zero gradients at the start of an accumulation cycle\n",
    "        if train and i % accumulation_steps == 0:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        if str(loader.dataset.device).startswith('cuda'):\n",
    "            with autocast(enabled=True):  # Mixed precision for CUDA\n",
    "                logit = model.forward(vol_device, original_slices)\n",
    "                loss = loader.dataset.weighted_loss(logit, label, train) / accumulation_steps\n",
    "        else:\n",
    "            logit = model.forward(vol_device, original_slices)\n",
    "            loss = loader.dataset.weighted_loss(logit, label, train) / accumulation_steps\n",
    "\n",
    "        # Backward pass for training\n",
    "        if train:\n",
    "            loss.backward()  # Accumulate gradients\n",
    "\n",
    "            # Update weights after accumulation_steps batches\n",
    "            if (i + 1) % accumulation_steps == 0:\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "        # Track loss (scale back for logging)\n",
    "        total_loss += loss.item() * accumulation_steps\n",
    "        num_batches += 1\n",
    "\n",
    "        # Collect predictions and labels\n",
    "        pred = torch.sigmoid(logit)\n",
    "        pred_npy = pred.data.cpu().numpy().flatten()\n",
    "        label_npy = label.data.cpu().numpy().flatten()\n",
    "        preds.extend(pred_npy)\n",
    "        labels.extend(label_npy)\n",
    "\n",
    "    # Handle remaining gradients at the end of the epoch\n",
    "    if train and num_batches % accumulation_steps != 0:\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    # Compute average loss and AUC\n",
    "    avg_loss = total_loss / num_batches\n",
    "    fpr, tpr, _ = metrics.roc_curve(labels, preds)\n",
    "    auc = metrics.auc(fpr, tpr)\n",
    "\n",
    "    return avg_loss, auc, preds, labels\n",
    "\n",
    "def evaluate(split, model_path, use_gpu, use_mps, data_dir, labels_csv, batch_size, label_smoothing):\n",
    "    device = get_device(use_gpu, use_mps)\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    if split == 'train' or split == 'valid':\n",
    "        train_loader, valid_loader = load_data3(device, data_dir, labels_csv, batch_size=batch_size, label_smoothing=label_smoothing)\n",
    "\n",
    "    elif split == 'test':\n",
    "        test_loader = load_data_test(device, data_dir, labels_csv, batch_size=batch_size, label_smoothing=label_smoothing)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"split must be 'train', 'valid', or 'test'\")\n",
    "    \n",
    "    print(\"Loading model from path:\", model_path)\n",
    "\n",
    "    model = MRNet3()\n",
    "    state_dict = torch.load(model_path, map_location=device)\n",
    "    model.load_state_dict(state_dict)\n",
    "    model = model.to(device)\n",
    "\n",
    "    if split == 'train':\n",
    "        loader = train_loader\n",
    "    elif split == 'valid':\n",
    "        loader = valid_loader\n",
    "    elif split == 'test':\n",
    "        loader = test_loader\n",
    "\n",
    "    loss, auc, preds, labels = run_model(model, loader, train=False)\n",
    "\n",
    "    print(f'{split} loss: {loss:0.4f}')\n",
    "    print(f'{split} AUC: {auc:0.4f}')\n",
    "\n",
    "    return preds, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a569db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from sklearn import metrics\n",
    "\n",
    "def get_device(use_gpu, use_mps):\n",
    "    if use_gpu and torch.cuda.is_available():\n",
    "        return torch.device(\"cuda\")\n",
    "    elif use_mps and torch.backends.mps.is_available():\n",
    "        return torch.device(\"mps\")\n",
    "    else:\n",
    "        return torch.device(\"cpu\")\n",
    "\n",
    "def train3(rundir, epochs, learning_rate, use_gpu, use_mps, data_dir, labels_csv, weight_decay, max_patience, batch_size, label_smoothing):\n",
    "    device = get_device(use_gpu, use_mps)\n",
    "    print(f\"Using device: {device}\")\n",
    "    train_loader, _ = load_data3(device, data_dir, labels_csv, batch_size=batch_size, label_smoothing=label_smoothing)\n",
    "    _, valid_loader = load_data3(device, data_dir, labels_csv, batch_size=1, label_smoothing=label_smoothing)\n",
    "\n",
    "    model = MRNet3()\n",
    "    model = model.to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=max_patience, factor=.3, threshold=1e-4)\n",
    "\n",
    "    best_val_auc = float('-inf')\n",
    "\n",
    "    start_time = datetime.now()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        change = datetime.now() - start_time\n",
    "        print('starting epoch {}. time passed: {}'.format(epoch+1, str(change)))\n",
    "        \n",
    "        train_loss, train_auc, _, _ = run_model(model, train_loader, train=True, optimizer=optimizer)\n",
    "        print(f'train loss: {train_loss:0.4f}')\n",
    "        print(f'train AUC: {train_auc:0.4f}')\n",
    "\n",
    "        val_loss, val_auc, _, _ = run_model(model, valid_loader, train=False)\n",
    "        print(f'valid loss: {val_loss:0.4f}')\n",
    "        print(f'valid AUC: {val_auc:0.4f}')\n",
    "\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        if val_auc > best_val_auc:\n",
    "            best_val_auc = val_auc\n",
    "\n",
    "            file_name = f'val{val_auc:0.4f}_train{train_auc:0.4f}_epoch{epoch+1}'\n",
    "            save_path = Path(rundir) / file_name\n",
    "            \n",
    "            print(f\"Saving model to {save_path}\")\n",
    "            \n",
    "            torch.save(model.state_dict(), save_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3385aa9c",
   "metadata": {},
   "source": [
    "<h3>Training</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a5d089",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rundir =  \"/Users/matteobruno/Desktop/runs\"  #\"directory/to/store/runs\"\n",
    "data_dir = \"/Users/matteobruno/Desktop/MRNet-v1.0/train\" #\"Directory/containing/.npy_files'\"\n",
    "labels_csv =  \"/Users/matteobruno/Desktop/MRNet-v1.0/train/train-acl.csv\" #\"Path/to/labels/CSV/file\"\n",
    "seed = 42\n",
    "gpu = False #If true runs on Nvidia GPU\n",
    "mps = True #If true runs on Apple MPS\n",
    "learning_rate = 1e-04\n",
    "weight_decay = 1e-05\n",
    "epochs = 50\n",
    "max_patience = 5\n",
    "batch_size = 4\n",
    "label_smoothing = 0.0 #Label smoothing factor (0.0 = no smoothing)'\n",
    "\n",
    "\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if gpu and torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "elif mps and torch.backends.mps.is_available():\n",
    "    pass\n",
    "\n",
    "os.makedirs(rundir, exist_ok=True)\n",
    "\n",
    "\n",
    "# Save parameters to args.json\n",
    "params = {\n",
    "    \"rundir\": rundir,\n",
    "    \"data_dir\": data_dir,\n",
    "    \"labels_csv\": labels_csv,\n",
    "    \"seed\": seed,\n",
    "    \"gpu\": gpu,\n",
    "    \"mps\": mps,\n",
    "    \"learning_rate\": learning_rate,\n",
    "    \"weight_decay\": weight_decay,\n",
    "    \"epochs\": epochs,\n",
    "    \"max_patience\": max_patience,\n",
    "    \"batch_size\": batch_size,\n",
    "    \"label_smoothing\": label_smoothing,\n",
    "}\n",
    "with open(Path(rundir) / 'args.json', 'w') as out:\n",
    "    json.dump(params, out, indent=4)\n",
    "    \n",
    "    train3(rundir, epochs, learning_rate, gpu, mps, data_dir, labels_csv, weight_decay, \n",
    "           max_patience, batch_size, label_smoothing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2974acf8",
   "metadata": {},
   "source": [
    "<h3>Testing</h3>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "455ecbea",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"/Users/matteobruno/Desktop/\"  # Path to the saved model\n",
    "split = \"test\"  # or \"train\", \"valid\"\n",
    "data_dir = \"/Users/matteobruno/Desktop/MRNet-v1.0/test\" #\"Directory/containing/.npy_files'\"\n",
    "labels_csv =  \"/Users/matteobruno/Desktop/MRNet-v1.0/test/valid-acl.csv\" #\"Path/to/labels/CSV/file\"\n",
    "gpu = False #If true runs on Nvidia GPU\n",
    "mps = True #If true runs on Apple MPS\n",
    "batch_size = 1\n",
    "label_smoothing = 0.0 #Label smoothing factor (0.0 = no smoothing)'\n",
    "\n",
    "evaluate(split, model_path, gpu, mps, data_dir, labels_csv, batch_size, label_smoothing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62fbb72c",
   "metadata": {},
   "source": [
    "<h1> Ensemble model</h1>\n",
    "Finally we decide to build an ensemble model with the two best model we got, that is Alexnet Majority Vote and Resnet. \n",
    "In this model each sample goes trough the Resent-based model convolutoinal layers and trough the ALexnet-based model convolutional layers. The two resulting feature vector are then concatenated and passed trough a three layers beep MLP to obtain the final probabilities.\n",
    "During training the weigths from the best Alexnet-based and Resnet-based models are loaded for the convolutional layers, which are then frozen. This allows us to only train the MLP layers which makes the task feasible on teh available hardware.\n",
    "A small performance from the Resnet model increase was obtained, with a validation AUC of 96.21% (train AUC of 98.25) and a testing AUC of 96.38%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a22fb8",
   "metadata": {},
   "source": [
    "<h3>Model class</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c73b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "from torchvision.models import AlexNet_Weights, ResNet18_Weights\n",
    "\n",
    "class MRNetAlex(nn.Module):\n",
    "    \"\"\"Model 1: AlexNet-based model with separate classifiers per view.\"\"\"\n",
    "    def __init__(self, use_batchnorm=False):\n",
    "        super().__init__()\n",
    "        self.model1 = models.alexnet(weights=AlexNet_Weights.DEFAULT)  # Axial\n",
    "        self.model2 = models.alexnet(weights=AlexNet_Weights.DEFAULT)  # Coronal\n",
    "        self.model3 = models.alexnet(weights=AlexNet_Weights.DEFAULT)  # Sagittal\n",
    "        self.gap = nn.AdaptiveMaxPool2d(1)\n",
    "        self.use_batchnorm = use_batchnorm\n",
    "        n = 0.15\n",
    "        self.dropout_view1 = nn.Dropout(p=n)\n",
    "        self.dropout_view2 = nn.Dropout(p=n)\n",
    "        self.dropout_view3 = nn.Dropout(p=n)\n",
    "\n",
    "        # Classifiers for each view\n",
    "        classifier_layers_axial = [nn.Linear(256, 256)]\n",
    "        if self.use_batchnorm:\n",
    "            classifier_layers_axial.append(nn.BatchNorm1d(256))\n",
    "        self.classifier1_axial = nn.Sequential(*classifier_layers_axial)\n",
    "        self.classifier1_coronal = nn.Sequential(*[nn.Linear(256, 256)] + ([nn.BatchNorm1d(256)] if self.use_batchnorm else []))\n",
    "        self.classifier1_sagittal = nn.Sequential(*[nn.Linear(256, 256)] + ([nn.BatchNorm1d(256)] if self.use_batchnorm else []))\n",
    "        self.classifier2_axial = nn.Linear(256, 1)\n",
    "        self.classifier2_coronal = nn.Linear(256, 1)\n",
    "        self.classifier2_sagittal = nn.Linear(256, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Not implemented as it's not needed for the ensemble\n",
    "        pass\n",
    "\n",
    "class MRNetResNet(nn.Module):\n",
    "    \"\"\"Model 2: ResNet18-based model with feature concatenation.\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model1 = models.resnet18(weights=ResNet18_Weights.DEFAULT)\n",
    "        self.model1 = nn.Sequential(*list(self.model1.children())[:-1])  # Axial\n",
    "        self.model2 = models.resnet18(weights=ResNet18_Weights.DEFAULT)\n",
    "        self.model2 = nn.Sequential(*list(self.model2.children())[:-1])  # Coronal\n",
    "        self.model3 = models.resnet18(weights=ResNet18_Weights.DEFAULT)\n",
    "        self.model3 = nn.Sequential(*list(self.model3.children())[:-1])  # Sagittal\n",
    "        self.gap = nn.AdaptiveAvgPool2d(1)\n",
    "        self.dropout_view1 = nn.Dropout(p=0.7)\n",
    "        self.dropout_view2 = nn.Dropout(p=0.7)\n",
    "        self.dropout_view3 = nn.Dropout(p=0.7)\n",
    "        self.classifier1 = nn.Linear(512 * 3, 256)\n",
    "        self.bn1 = nn.BatchNorm1d(256)\n",
    "        self.dropout = nn.Dropout(p=0.4)\n",
    "        self.activation = nn.ReLU()\n",
    "        self.classifier2 = nn.Linear(256, 1)\n",
    "\n",
    "    def forward(self, x, original_slices):\n",
    "        # Not implemented as it's not needed for the ensemble\n",
    "        pass\n",
    "\n",
    "class EnsembleMRNet(nn.Module):\n",
    "    \"\"\"Ensemble model combining CNNs from MRNetAlex and MRNetResNet with a new dense classifier.\"\"\"\n",
    "    def __init__(self, model1_path=None, model2_path=None, device=\"cpu\"):\n",
    "        super().__init__()\n",
    "        # Initialize base models\n",
    "        self.model_alex = MRNetAlex()\n",
    "        self.model_resnet = MRNetResNet()\n",
    "        \n",
    "        # Load pre-trained weights\n",
    "        if model1_path is not None:\n",
    "            self.model_alex.load_state_dict(torch.load(model1_path, map_location=device, weights_only=True))\n",
    "        if model2_path is not None:\n",
    "            self.model_resnet.load_state_dict(torch.load(model2_path, map_location=device, weights_only=True))\n",
    "        \n",
    "        # Freeze CNN parts\n",
    "        for model in [self.model_alex.model1, self.model_alex.model2, self.model_alex.model3]:\n",
    "            for param in model.features.parameters():\n",
    "                param.requires_grad = False\n",
    "        for model in [self.model_resnet.model1, self.model_resnet.model2, self.model_resnet.model3]:\n",
    "            for param in model.parameters():\n",
    "                param.requires_grad = False\n",
    "        \n",
    "        # Define pooling layers consistent with original models\n",
    "        self.gap_max = nn.AdaptiveMaxPool2d(1)  # For AlexNet\n",
    "        self.gap_avg = nn.AdaptiveAvgPool2d(1)  # For ResNet18\n",
    "        \n",
    "        # Define dropout layers for each backbone per view\n",
    "        self.dropout_alex_view1 = nn.Dropout(p=0.15)\n",
    "        self.dropout_alex_view2 = nn.Dropout(p=0.15)\n",
    "        self.dropout_alex_view3 = nn.Dropout(p=0.15)\n",
    "        self.dropout_resnet_view1 = nn.Dropout(p=0.7)\n",
    "        self.dropout_resnet_view2 = nn.Dropout(p=0.7)\n",
    "        self.dropout_resnet_view3 = nn.Dropout(p=0.7)\n",
    "        \n",
    "        # New dense classifier\n",
    "        self.dense = nn.Sequential(\n",
    "            nn.Linear(2304, 1024),  # Input: 2304, Output: 512           \n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.5),\n",
    "            ###wasn't there in the best model \n",
    "            nn.Linear(1024, 512),   # Input: 1024, Output: 512\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.3),\n",
    "            ###wasn't there in the best model\n",
    "            nn.Linear(512, 1)      # Output: 1 for binary classification\n",
    "        )\n",
    "\n",
    "    def forward(self, padded_views, original_slices):\n",
    "        \"\"\"Forward pass: Extract features from both CNNs, concatenate, and classify.\"\"\"\n",
    "        B = padded_views[0].shape[0]\n",
    "        view_features = []\n",
    "        \n",
    "        for view in range(3):\n",
    "            x_view = padded_views[view]  # [B, S_max, 3, 224, 224]\n",
    "            S_max = x_view.shape[1]\n",
    "            x_view_flat = x_view.view(B * S_max, 3, 224, 224)\n",
    "            \n",
    "            # AlexNet features\n",
    "            if view == 0:\n",
    "                feat_alex = self.model_alex.model1.features(x_view_flat)  # [B*S_max, 256, 6, 6]\n",
    "            elif view == 1:\n",
    "                feat_alex = self.model_alex.model2.features(x_view_flat)\n",
    "            else:\n",
    "                feat_alex = self.model_alex.model3.features(x_view_flat)\n",
    "            feat_alex = self.gap_max(feat_alex).view(B, S_max, 256)\n",
    "            mask = torch.arange(S_max, device=feat_alex.device).expand(B, S_max) < original_slices[view].unsqueeze(1)\n",
    "            feat_alex = feat_alex.masked_fill(~mask.unsqueeze(2), -float('inf'))\n",
    "            max_feat_alex = torch.max(feat_alex, dim=1)[0]  # [B, 256]\n",
    "            \n",
    "            # ResNet18 features\n",
    "            if view == 0:\n",
    "                feat_resnet = self.model_resnet.model1(x_view_flat)  # [B*S_max, 512, 7, 7]\n",
    "            elif view == 1:\n",
    "                feat_resnet = self.model_resnet.model2(x_view_flat)\n",
    "            else:\n",
    "                feat_resnet = self.model_resnet.model3(x_view_flat)\n",
    "            feat_resnet = self.gap_avg(feat_resnet).view(B, S_max, 512)\n",
    "            feat_resnet = feat_resnet.masked_fill(~mask.unsqueeze(2), -float('inf'))\n",
    "            max_feat_resnet = torch.max(feat_resnet, dim=1)[0]  # [B, 512]\n",
    "            \n",
    "            # Apply dropout\n",
    "            if view == 0:\n",
    "                max_feat_alex = self.dropout_alex_view1(max_feat_alex)\n",
    "                max_feat_resnet = self.dropout_resnet_view1(max_feat_resnet)\n",
    "            elif view == 1:\n",
    "                max_feat_alex = self.dropout_alex_view2(max_feat_alex)\n",
    "                max_feat_resnet = self.dropout_resnet_view2(max_feat_resnet)\n",
    "            else:\n",
    "                max_feat_alex = self.dropout_alex_view3(max_feat_alex)\n",
    "                max_feat_resnet = self.dropout_resnet_view3(max_feat_resnet)\n",
    "            \n",
    "            # Concatenate features for this view\n",
    "            combined_feat = torch.cat([max_feat_alex, max_feat_resnet], dim=1)  # [B, 768]\n",
    "            view_features.append(combined_feat)\n",
    "        \n",
    "        # Concatenate all views\n",
    "        all_features = torch.cat(view_features, dim=1)  # [B, 2304]\n",
    "        \n",
    "        # Dense classifier\n",
    "        logits = self.dense(all_features)  # [B, 1]\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c43d501",
   "metadata": {},
   "source": [
    "<h3>Loader</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e2c4d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import kornia.augmentation as K\n",
    "import random\n",
    "\n",
    "INPUT_DIM = 224\n",
    "MAX_PIXEL_VAL = 1.0\n",
    "MEAN = [0.485, 0.456, 0.406]\n",
    "STDDEV = [0.229, 0.224, 0.225]\n",
    "\n",
    "class MRDataset(data.Dataset):\n",
    "    def __init__(self, data_dir, file_list, labels_dict, device, label_smoothing=0.1, is_training=False):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.is_training = is_training\n",
    "        self.data_dir_axial = f\"{data_dir}/axial\"\n",
    "        self.data_dir_coronal = f\"{data_dir}/coronal\"\n",
    "        self.data_dir_sagittal = f\"{data_dir}/sagittal\"\n",
    "        self.paths_axial = [os.path.join(self.data_dir_axial, file) for file in file_list]\n",
    "        self.paths_coronal = [os.path.join(self.data_dir_coronal, file) for file in file_list]\n",
    "        self.paths_sagittal = [os.path.join(self.data_dir_sagittal, file) for file in file_list]\n",
    "        self.paths = [self.paths_axial, self.paths_coronal, self.paths_sagittal]\n",
    "        self.labels = [labels_dict[file] for file in file_list]\n",
    "        self.label_smoothing = label_smoothing\n",
    "        neg_weight = np.mean(self.labels)\n",
    "        self.weights = [float(neg_weight), float(1 - neg_weight)]\n",
    "\n",
    "    def weighted_loss(self, prediction, target, train):\n",
    "        indices = target.squeeze(1).long()\n",
    "        weights_tensor = torch.tensor(self.weights, device=self.device, dtype=torch.float32)[indices].unsqueeze(1)\n",
    "        smoothed_target = target * (1 - self.label_smoothing) + (1 - target) * self.label_smoothing if train and self.label_smoothing > 0 else target\n",
    "        loss = F.binary_cross_entropy_with_logits(prediction, smoothed_target, weight=weights_tensor)\n",
    "        return loss\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        vol_list = []\n",
    "        for i in range(3):\n",
    "            path = self.paths[i][index]\n",
    "            vol = np.load(path).astype(np.float32)\n",
    "            pad = int((vol.shape[2] - INPUT_DIM) / 2)\n",
    "            vol = vol[:, pad:-pad, pad:-pad]\n",
    "            vol = (vol - np.min(vol)) / (np.max(vol) - np.min(vol) + 1e-6)\n",
    "            vol = np.stack((vol,) * 3, axis=1)\n",
    "            vol_tensor = torch.FloatTensor(vol).to(self.device)\n",
    "            \n",
    "            # Apply augmentations only during training\n",
    "            if self.is_training and random.random() < 0.3:\n",
    "                vol_tensor = self.apply_augmentations(vol_tensor)\n",
    "\n",
    "            for c in range(3):\n",
    "                vol_tensor[:, c, :, :] = (vol_tensor[:, c, :, :] - MEAN[c]) / STDDEV[c]\n",
    "            vol_list.append(vol_tensor)\n",
    "        label_tensor = torch.FloatTensor([self.labels[index]]).to(self.device)\n",
    "        return vol_list, label_tensor\n",
    "\n",
    "    def apply_augmentations(self, vol_tensor):\n",
    "        # vol_tensor shape: [slices, channels, height, width]\n",
    "        # Reshape to treat slices as batch dimension\n",
    "        vol_tensor = vol_tensor.permute(1, 0, 2, 3)  # [channels, slices, height, width]\n",
    "        \n",
    "        # Apply augmentations\n",
    "        vol_tensor = K.RandomRotation(degrees=25, keepdim=True)(vol_tensor)\n",
    "        vol_tensor = K.RandomAffine(degrees=0, translate=(25/224, 25/224), keepdim=True)(vol_tensor)\n",
    "        if random.random() > 0.5:\n",
    "            vol_tensor = K.RandomHorizontalFlip(p=1.0, keepdim=True)(vol_tensor)\n",
    "        \n",
    "        # Reshape back to [slices, channels, height, width]\n",
    "        vol_tensor = vol_tensor.permute(1, 0, 2, 3)\n",
    "        return vol_tensor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    device = batch[0][0][0].device\n",
    "    view0_list = [sample[0][0] for sample in batch]\n",
    "    view1_list = [sample[0][1] for sample in batch]\n",
    "    view2_list = [sample[0][2] for sample in batch]\n",
    "    padded_view0 = torch.nn.utils.rnn.pad_sequence(view0_list, batch_first=True)\n",
    "    padded_view1 = torch.nn.utils.rnn.pad_sequence(view1_list, batch_first=True)\n",
    "    padded_view2 = torch.nn.utils.rnn.pad_sequence(view2_list, batch_first=True)\n",
    "    original_slices0 = torch.tensor([v.shape[0] for v in view0_list], device=device)\n",
    "    original_slices1 = torch.tensor([v.shape[0] for v in view1_list], device=device)\n",
    "    original_slices2 = torch.tensor([v.shape[0] for v in view2_list], device=device)\n",
    "    labels = torch.stack([sample[1] for sample in batch])\n",
    "    return [padded_view0, padded_view1, padded_view2], labels, [original_slices0, original_slices1, original_slices2]\n",
    "\n",
    "def load_data3(device, data_dir, labels_csv, batch_size=1, label_smoothing=0.1):\n",
    "    labels_df = pd.read_csv(labels_csv, header=None, names=['filename', 'label'])\n",
    "    labels_df['filename'] = labels_df['filename'].apply(lambda x: f\"{int(x):04d}.npy\")\n",
    "    labels_dict = dict(zip(labels_df['filename'], labels_df['label']))\n",
    "    all_files = [f for f in os.listdir(f\"{data_dir}/axial\") if f.endswith(\".npy\") and f in labels_dict]\n",
    "    all_files.sort()\n",
    "    labels = [labels_dict[file] for file in all_files]\n",
    "    train_files, valid_files = train_test_split(\n",
    "        all_files, test_size=0.2, random_state=42, stratify=labels\n",
    "    )\n",
    "    train_dataset = MRDataset(data_dir, train_files, labels_dict, device, label_smoothing, is_training=True)\n",
    "    valid_dataset = MRDataset(data_dir, valid_files, labels_dict, device, label_smoothing, is_training=False)\n",
    "    train_loader = data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0, collate_fn=collate_fn)\n",
    "    valid_loader = data.DataLoader(valid_dataset, batch_size=batch_size, shuffle=False, num_workers=0, collate_fn=collate_fn)\n",
    "    return train_loader, valid_loader\n",
    "\n",
    "def load_data_test(device, data_dir, labels_csv, batch_size=1, label_smoothing=0.1):\n",
    "    labels_df = pd.read_csv(labels_csv, header=None, names=['filename', 'label'])\n",
    "    labels_df['filename'] = labels_df['filename'].apply(lambda x: f\"{int(x):04d}.npy\")\n",
    "    labels_dict = dict(zip(labels_df['filename'], labels_df['label']))\n",
    "    test_files = [f for f in os.listdir(f\"{data_dir}/axial\") if f.endswith(\".npy\") and f in labels_dict]\n",
    "    test_files.sort()\n",
    "    test_dataset = MRDataset(data_dir, test_files, labels_dict, device, label_smoothing, is_training=False)\n",
    "    test_loader = data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=0, collate_fn=collate_fn)\n",
    "    return test_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8cce0be",
   "metadata": {},
   "source": [
    "<h3>Training and evaluation functions</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc3b7f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn import metrics\n",
    "from torch.autograd import Variable\n",
    "from tqdm import tqdm\n",
    "from torch.amp import autocast\n",
    "\n",
    "def get_device(use_gpu, use_mps):\n",
    "    if use_gpu and torch.cuda.is_available():\n",
    "        return torch.device(\"cuda\")\n",
    "    elif use_mps and torch.backends.mps.is_available():\n",
    "        return torch.device(\"mps\")\n",
    "    else:\n",
    "        return torch.device(\"cpu\")\n",
    "\n",
    "def run_model(model, loader, train=False, optimizer=None):\n",
    "    preds = []\n",
    "    labels = []\n",
    "    model.train() if train else model.eval()\n",
    "    total_loss = 0.0\n",
    "    num_batches = 0\n",
    "    for batch in tqdm(loader, desc=\"Processing batches\", total=len(loader)):\n",
    "        if train:\n",
    "            optimizer.zero_grad()\n",
    "        padded_views, label, original_slices = batch\n",
    "        label = label.to(loader.dataset.device)\n",
    "        if str(loader.dataset.device).startswith('cuda'):\n",
    "            with autocast(device_type='cuda', enabled=True):\n",
    "                logit = model(padded_views, original_slices)\n",
    "                loss = loader.dataset.weighted_loss(logit, label, train)\n",
    "        else:\n",
    "            logit = model(padded_views, original_slices)\n",
    "            loss = loader.dataset.weighted_loss(logit, label, train)\n",
    "        total_loss += loss.item()\n",
    "        pred = torch.sigmoid(logit)\n",
    "        pred_npy = pred.data.cpu().numpy().flatten()\n",
    "        label_npy = label.data.cpu().numpy().flatten()\n",
    "        preds.extend(pred_npy)\n",
    "        labels.extend(label_npy)\n",
    "        if train:\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        num_batches += 1\n",
    "    avg_loss = total_loss / num_batches\n",
    "    fpr, tpr, _ = metrics.roc_curve(labels, preds)\n",
    "    auc = metrics.auc(fpr, tpr)\n",
    "    return avg_loss, auc, preds, labels\n",
    "\n",
    "def evaluate(split, model_path, use_gpu, use_mps, data_dir, labels_csv, batch_size, label_smoothing):\n",
    "    device = get_device(use_gpu, use_mps)\n",
    "    print(f\"Using device: {device}\")\n",
    "    if split in ['train', 'valid']:\n",
    "        train_loader, valid_loader = load_data3(device, data_dir, labels_csv, batch_size=batch_size, label_smoothing=label_smoothing)\n",
    "        loader = train_loader if split == 'train' else valid_loader\n",
    "    elif split == 'test':\n",
    "        loader = load_data_test(device, data_dir, labels_csv, batch_size=batch_size, label_smoothing=label_smoothing)\n",
    "    else:\n",
    "        raise ValueError(\"split must be 'train', 'valid', or 'test'\")\n",
    "    model = EnsembleMRNet(device=device)\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model = model.to(device)\n",
    "    loss, auc, preds, labels = run_model(model, loader, train=False)\n",
    "    print(f'{split} loss: {loss:.4f}')\n",
    "    print(f'{split} AUC: {auc:.4f}')\n",
    "    return preds, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e38b6a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from sklearn import metrics\n",
    "\n",
    "def get_device(use_gpu, use_mps):\n",
    "    if use_gpu and torch.cuda.is_available():\n",
    "        return torch.device(\"cuda\")\n",
    "    elif use_mps and torch.backends.mps.is_available():\n",
    "        return torch.device(\"mps\")\n",
    "    else:\n",
    "        return torch.device(\"cpu\")\n",
    "\n",
    "def train(rundir, model1_path, model2_path, epochs, learning_rate, use_gpu, use_mps, data_dir, labels_csv, weight_decay, max_patience, batch_size, label_smoothing):\n",
    "    device = get_device(use_gpu, use_mps)\n",
    "    print(f\"Using device: {device}\")\n",
    "    train_loader, valid_loader = load_data3(device, data_dir, labels_csv, batch_size=batch_size, label_smoothing=label_smoothing)\n",
    "    model = EnsembleMRNet(model1_path, model2_path, device).to(device)\n",
    "    optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=learning_rate, weight_decay=weight_decay)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=max_patience, factor=0.3, threshold=1e-4)\n",
    "    best_val_auc = float('-inf')\n",
    "    start_time = datetime.now()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        change = datetime.now() - start_time\n",
    "        print(f'Starting epoch {epoch+1}. Time passed: {change}')\n",
    "        train_loss, train_auc, _, _ = run_model(model, train_loader, train=True, optimizer=optimizer)\n",
    "        print(f'Train loss: {train_loss:.4f}, Train AUC: {train_auc:.4f}')\n",
    "        val_loss, val_auc, _, _ = run_model(model, valid_loader, train=False)\n",
    "        print(f'Valid loss: {val_loss:.4f}, Valid AUC: {val_auc:.4f}')\n",
    "        scheduler.step(val_loss)\n",
    "        if val_auc > best_val_auc:\n",
    "            best_val_auc = val_auc\n",
    "            file_name = f'val{val_auc:.4f}_train{train_auc:.4f}_epoch{epoch+1}'\n",
    "            save_path = Path(rundir) / file_name\n",
    "            torch.save(model.state_dict(), save_path)\n",
    "            print(f\"Model saved to {save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b98ee60",
   "metadata": {},
   "source": [
    "<h3>Training</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c53384",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rundir =  \"/Users/matteobruno/Desktop/runs\"  #\"directory/to/store/runs\"\n",
    "model1_path = \"/Users/matteobruno/Desktop/Best_alexnet_majority/best_model.pth\" #path to pre-trained AlexNet model\n",
    "model2_path = \"/Users/matteobruno/Desktop/Best_resnet/val0.9547_train0.9974_epoch26\" #path to pre-trained ResNet model\n",
    "data_dir = \"/Users/matteobruno/Desktop/MRNet-v1.0/train\" #\"Directory/containing/.npy_files'\"\n",
    "labels_csv =  \"/Users/matteobruno/Desktop/MRNet-v1.0/train/train-acl.csv\" #\"Path/to/labels/CSV/file\"\n",
    "seed = 42\n",
    "gpu = False #If true runs on Nvidia GPU\n",
    "mps = True #If true runs on Apple MPS\n",
    "learning_rate = 1e-04\n",
    "weight_decay = 1e-05\n",
    "epochs = 50\n",
    "max_patience = 5\n",
    "batch_size = 4\n",
    "label_smoothing = 0.0 #Label smoothing factor (0.0 = no smoothing)'\n",
    "\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if gpu and torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "elif mps and torch.backends.mps.is_available():\n",
    "    pass\n",
    "\n",
    "os.makedirs(rundir, exist_ok=True)\n",
    "\n",
    "\n",
    "# Save parameters to args.json\n",
    "params = {\n",
    "    \"rundir\": rundir,\n",
    "    \"model_path\": model1_path,\n",
    "    \"model2_path\": model2_path,\n",
    "    \"data_dir\": data_dir,\n",
    "    \"labels_csv\": labels_csv,\n",
    "    \"seed\": seed,\n",
    "    \"gpu\": gpu,\n",
    "    \"mps\": mps,\n",
    "    \"learning_rate\": learning_rate,\n",
    "    \"weight_decay\": weight_decay,\n",
    "    \"epochs\": epochs,\n",
    "    \"max_patience\": max_patience,\n",
    "    \"batch_size\": batch_size,\n",
    "    \"label_smoothing\": label_smoothing,\n",
    "}\n",
    "with open(Path(rundir) / 'args.json', 'w') as out:\n",
    "    json.dump(params, out, indent=4)\n",
    "    \n",
    "    train(rundir, model1_path, model2_path, epochs, learning_rate, gpu, mps, data_dir, labels_csv, weight_decay, \n",
    "           max_patience, batch_size, label_smoothing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1772846c",
   "metadata": {},
   "source": [
    "<h3>Testing</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "804fc161",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"/Users/matteobruno/Desktop/Ensemble_best/val0.9621_train0.9895_epoch20\"  # Path to the saved model\n",
    "split = \"test\"  # or \"train\", \"valid\"\n",
    "data_dir = \"/Users/matteobruno/Desktop/MRNet-v1.0/test\" #\"Directory/containing/.npy_files'\"\n",
    "labels_csv =  \"/Users/matteobruno/Desktop/MRNet-v1.0/test/valid-acl.csv\" #\"Path/to/labels/CSV/file\"\n",
    "gpu = False #If true runs on Nvidia GPU\n",
    "mps = True #If true runs on Apple MPS\n",
    "batch_size = 1\n",
    "label_smoothing = 0.0 #Label smoothing factor (0.0 = no smoothing)'\n",
    "\n",
    "evaluate(split, model_path, gpu, mps, data_dir, labels_csv, batch_size, label_smoothing)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mrnet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
